#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ“ å–®ä»»å‹™åˆ†é¡æ¨¡å‹ (pointId)ï¼šåƒ…é æ¸¬ pointId
---------------------------------------------------------------------
ğŸŒŸ ä¾†æºï¼š
- å¾ v3 actionId æ¨¡å‹ä¿®æ”¹è€Œä¾†ï¼Œå°ˆæ³¨æ–¼ pointId é æ¸¬ã€‚
- ä¿ç•™äº† v2 çš„æ»¯å¾Œç‰¹å¾µ (prev_1, prev_2, prev_3) å’Œ score_diff ç‰¹å¾µã€‚

â­ v3 æ›´æ–° (ç¹¼æ‰¿)ï¼š
- æ•´åˆ RandomizedSearchCV é€²è¡Œè¶…åƒæ•¸èª¿å„ªã€‚
- ä¿ç•™ Group Split (ä½¿ç”¨ PredefinedSplit)ã€‚
- æ•´åˆ class_weight ('balanced') è™•ç†ä¸å¹³è¡¡å•é¡Œã€‚
- æ•´åˆ Early Stopping æå‡æœå°‹æ•ˆç‡ã€‚

ğŸ Debug ç­†è¨˜ (ç¹¼æ‰¿)ï¼š
1.  **[æœ€å¯èƒ½çš„éŒ¯èª¤] UnicodeEncodeError**ï¼š
    å¦‚æœä½ çš„ä½œæ¥­ç³»çµ± (ç‰¹åˆ¥æ˜¯ Windows) çš„
    consoleï¼ˆå‘½ä»¤æç¤ºå­—å…ƒï¼‰é è¨­ç·¨ç¢¼ä¸æ˜¯ UTF-8ï¼Œ
    åŸ·è¡Œ `print("âœ… æœå°‹å®Œæˆ!")` 
    é€™é¡åŒ…å«ä¸­æ–‡çš„æŒ‡ä»¤æ™‚ï¼Œå¯èƒ½æœƒå¼•ç™¼ `UnicodeEncodeError`ã€‚

    **è§£æ±ºæ–¹æ³•**ï¼š
    åœ¨åŸ·è¡Œæ­¤è…³æœ¬å‰ï¼Œå…ˆåœ¨ä½ çš„çµ‚ç«¯æ©Ÿè¨­å®šç’°å¢ƒè®Šæ•¸ï¼š
    - (Windows CMD): `set PYTHONIOENCODING=utf-8`
    - (Windows PowerShell): `$env:PYTHONIOENCODING = "utf-8"`
    - (Linux/macOS): `export PYTHONIOENCODING=utf-8`
    ç„¶å¾Œå†åŸ·è¡Œ `python your_script_name.py`
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split, RandomizedSearchCV, PredefinedSplit
from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.feature_selection import VarianceThreshold
from sklearn.utils.class_weight import compute_sample_weight
import sys
import matplotlib.pyplot as plt

def plot_and_print_confusion_matrix(y_true, y_pred, title="pointId æ··æ·†çŸ©é™£"):
    """
    å°å‡ºä¸¦ç¹ªè£½æ··æ·†çŸ©é™£
    """
    cm = confusion_matrix(y_true, y_pred)
    print(f"\n{title}ï¼š\n", cm)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap=plt.cm.Blues)
    plt.title(title)
    plt.show()

# =========================================================
# 1ï¸âƒ£ è³‡æ–™è®€å– (ç„¡è®Šå‹•)
# =========================================================
def load_data(train_path="train.csv", test_path="test.csv"):
    """è®€å–è¨“ç·´é›†å’Œæ¸¬è©¦é›†"""
    try:
        train = pd.read_csv(train_path)
        test = pd.read_csv(test_path)
        print(f"âœ… Train shape: {train.shape}")
        print(f"âœ… Test shape: {test.shape}")
        return train, test
    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° {train_path} æˆ– {test_path}ã€‚è«‹ç¢ºä¿æª”æ¡ˆåœ¨æ­£ç¢ºçš„è·¯å¾‘ã€‚")
        sys.exit(1)

# =========================================================
# 2ï¸âƒ£ ç‰¹å¾µå·¥ç¨‹ (ç„¡è®Šå‹•)
# =========================================================
def create_features(df):
    """ç‚º train å’Œ test æ•¸æ“šé›†å»ºç«‹æ–°çš„åºåˆ—ç‰¹å¾µ (æ»¯å¾Œç‰¹å¾µ)"""
    df_new = df.copy()
    
    # ç¢ºä¿è³‡æ–™æŒ‰å›åˆå’Œæ‹æ•¸æ’åº
    df_new = df_new.sort_values(by=['rally_uid', 'strickNumber'])
    
    # 1. æ»¯å¾Œç‰¹å¾µ (Lag Features)
    lag_cols = ['actionId', 'pointId', 'spinId', 'strengthId', 'positionId']
    
    print(f"   > æ­£åœ¨å»ºç«‹ N-1, N-2, N-3 æ»¯å¾Œç‰¹å¾µ...")
    for col in lag_cols:
        for n in [1, 2, 3]:
            df_new[f'prev_{n}_{col}'] = df_new.groupby('rally_uid')[col].shift(n)

    # 2. æƒ…å¢ƒç‰¹å¾µ (Context Features) - åˆ†æ•¸
    df_new['score_diff'] = df_new['scoreSelf'] - df_new['scoreOther']

    # å¡«å…… shift() ç”¢ç”Ÿçš„ NaNs
    fill_cols = [col for col in df_new.columns if 'prev_' in col]
    df_new[fill_cols] = df_new[fill_cols].fillna(-1) 

    return df_new

# =========================================================
# 3ï¸âƒ£ é è™•ç† (*** æ¢å¾©-1è™•ç† ***)
# =========================================================
def preprocess(train_df, test_df):
    """
    1. (*** æ¢å¾© ***) ä¿®æ­£ pointId çš„ -1 é¡åˆ¥å•é¡Œ
    2. å–å¾—æ¸¬è©¦é›†æœ€å¾Œä¸€ç­†è³‡æ–™
    """
    # é æ¸¬æ™‚ï¼šä½¿ç”¨æ¯å€‹ rally_uid çš„ "æœ€å¾Œä¸€ç­†" è³‡æ–™
    test_last_shot = test_df.groupby('rally_uid').tail(1).copy()
    print(f"âœ… Test (last shots) shape: {test_last_shot.shape}")

    # (*** æ¢å¾© ***)
    # ä¿®æ­£ -1 é¡åˆ¥ (é‡å° pointId)
    original_max_label = None
    if (train_df["pointId"] == -1).any():
        max_label = train_df["pointId"].max()
        original_max_label = max_label + 1
        print(f"âš ï¸ pointId å«æœ‰ -1ï¼Œå°‡å…¶æ›¿æ›ç‚º {original_max_label}")
        train_df["pointId"] = train_df["pointId"].replace(-1, original_max_label)
    
    return train_df, test_last_shot, original_max_label

# =========================================================
# 4ï¸âƒ£ å»ºç«‹è¨“ç·´ä»»å‹™ (N -> N+1) (ç„¡è®Šå‹•)
# =========================================================
def create_training_data(train_df, feature_cols):
    """
    é‡æ–°å®šç¾©è¨“ç·´ä»»å‹™ (N -> N+1)
    - ç‰¹å¾µ (X) æ˜¯ç•¶å‰æ“Šçƒ (Shot N)
    - æ¨™ç±¤ (y) æ˜¯ "ä¸‹ä¸€çƒ" çš„ pointId (Shot N+1) (*** ä¿®æ”¹ ***)
    """
    # ç‰¹å¾µ (X) æ˜¯ç•¶å‰æ“Šçƒ (Shot N)
    X = train_df[feature_cols].copy().fillna(0)

    # (*** ä¿®æ”¹ ***) 
    # æ¨™ç±¤ (y) æ˜¯ "ä¸‹ä¸€çƒ" (Shot N+1) çš„ pointId
    y = train_df.groupby('rally_uid')['pointId'].shift(-1)
    
    # å„²å­˜ rally_uid ä»¥ä¾¿é€²è¡Œ group split
    rally_uids_for_split = train_df['rally_uid']

    # åˆªé™¤æ²’æœ‰ "ä¸‹ä¸€çƒ" çš„è¡Œ (å³æ¯å€‹å›åˆçš„æœ€å¾Œä¸€çƒ)
    valid_indices = y.notna()
    X = X[valid_indices]
    y = y[valid_indices]
    rally_uids_for_split = rally_uids_for_split[valid_indices]

    print(f"âœ… é‡æ–°å»ºç«‹è¨“ç·´é›† (N -> N+1)ï¼Œæ–° shape: {X.shape}")
    
    return X, y.astype(int), rally_uids_for_split

# =========================================================
# 5ï¸âƒ£ å»ºç«‹ç„¡æ´©æ¼çš„é©—è­‰é›† (Group Split) (ç„¡è®Šå‹•)
# =========================================================
def create_group_split(X, y, rally_uids):
    """
    ä½¿ç”¨ Group Split å»ºç«‹ç„¡æ´©æ¼çš„é©—è­‰é›†
    """
    print("ğŸ§© å»ºç«‹ç„¡æ´©æ¼çš„é©—è­‰é›†ä¸­ (Group Split)...")
    unique_rallies = rally_uids.unique()
    train_rallies, valid_rallies = train_test_split(unique_rallies, test_size=0.2, random_state=42)

    train_mask = rally_uids.isin(train_rallies)
    valid_mask = rally_uids.isin(valid_rallies)

    X_train, X_valid = X[train_mask], X[valid_mask]
    y_train, y_valid = y[train_mask], y[valid_mask]
    
    return X_train, X_valid, y_train, y_valid

# =========================================================
# 6ï¸âƒ£ ç‰¹å¾µé¸å– (ç„¡è®Šå‹•)
# =========================================================
def select_features(X, y, objective, num_class=None, top_k=30):
    """
    ä½¿ç”¨ XGBoost å…ˆè¨“ç·´ä¸€è¼ªï¼Œé¸å‡ºæœ€é‡è¦çš„å‰ K å€‹ç‰¹å¾µã€‚
    """
    selector = VarianceThreshold(threshold=0.0)
    X_var = selector.fit_transform(X)
    selected_cols = X.columns[selector.get_support()]
    
    X_var = pd.DataFrame(X_var, columns=selected_cols, index=X.index)

    model_params = {
        "objective": objective,
        "eval_metric": "mlogloss",
        "learning_rate": 0.1, "max_depth": 5, "n_estimators": 100,
        "subsample": 0.8, "colsample_bytree": 0.8,
        "random_state": 42, "tree_method": "hist",
        "num_class": num_class
    }

    model_tmp = xgb.XGBClassifier(**model_params)
    model_tmp.fit(X_var, y)

    importances = model_tmp.feature_importances_
    importance_df = pd.DataFrame({
        "feature": selected_cols,
        "importance": importances
    }).sort_values("importance", ascending=False)

    top_features = importance_df.head(top_k)["feature"].tolist()
    return top_features

# =========================================================
# 7ï¸âƒ£ æ¨™ç±¤é‚„åŸ (*** æ¢å¾© ***)
# =========================================================
def revert_negative(pred, replacement_val):
    """å°‡ max+1 é¡åˆ¥è½‰å› -1"""
    if replacement_val is not None:
        pred = pd.Series(pred)
        pred[pred == replacement_val] = -1
        return pred.values
    return pred

# =========================================================
# 8ï¸âƒ£ è¼¸å‡º submission.csv (ç„¡è®Šå‹•)
# =========================================================
def save_submission(test_last_shot, pred_point, sample_path="sample_submission.csv", output_path="submission.csv"):
    """
    è®€å– sample_submissionï¼Œåƒ…æ›´æ–° pointId æ¬„ä½å¾Œå„²å­˜
    """
    try:
        submission = pd.read_csv(sample_path)
    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° {sample_path}ã€‚ç„¡æ³•å»ºç«‹æäº¤æª”æ¡ˆã€‚")
        return
        
    # ç¢ºä¿ rally_uid å°é½Š
    submission = submission.set_index('rally_uid')
    
    # å»ºç«‹ä¸€å€‹åŒ…å«é æ¸¬çµæœçš„ Seriesï¼Œä¸¦ä»¥ rally_uid ç‚ºç´¢å¼•
    pred_df = pd.DataFrame({
        "rally_uid": test_last_shot["rally_uid"],
        "pointId": pred_point  # (*** ä¿®æ”¹ ***)
    }).set_index('rally_uid')

    # (*** ä¿®æ”¹ ***)
    # æ›´æ–° pointId æ¬„ä½
    submission['pointId'].update(pred_df['pointId'])
    submission['pointId'] = submission['pointId'].astype(int)

    # æ¢å¾©ç´¢å¼•ä¸¦å„²å­˜
    submission.reset_index(inplace=True)
    submission.to_csv(output_path, index=False)
    print(f"\nâœ… å·²è¼¸å‡º {output_path}")
    print(f"Submission shape: {submission.shape}")
    print(submission.head())

# =========================================================
# ğŸš€ ä¸»åŸ·è¡Œæµç¨‹
# =========================================================
def main():
    # --- åƒæ•¸è¨­å®š ---
    K_FEATURES = 20
    N_ITER_SEARCH = 25 # RandomizedSearch çš„æœå°‹æ¬¡æ•¸
    TRAIN_PATH = "train.csv"
    TEST_PATH = "test.csv"
    SAMPLE_SUB_PATH = "sample_submission.csv"
    SUBMISSION_PATH = "submission.csv" # (*** ä¿®æ”¹ ***)

    # --- 1. è®€å–è³‡æ–™ ---
    train, test = load_data(TRAIN_PATH, TEST_PATH)

    # --- 2. ç‰¹å¾µå·¥ç¨‹ ---
    print("âš™ï¸ æ­£åœ¨ç‚º train å»ºç«‹æ»¯å¾Œç‰¹å¾µ...")
    train = create_features(train)
    print("âš™ï¸ æ­£åœ¨ç‚º test å»ºç«‹æ»¯å¾Œç‰¹å¾µ...")
    test = create_features(test)

    # --- 3. é è™•ç† ---
    target_cols = ["actionId", "pointId", "serverGetPoint"]
    drop_cols = ["rally_uid", "rally_id"] 
    feature_cols = [c for c in train.columns if c not in target_cols + drop_cols and c in test.columns]
    print(f"âœ… ä½¿ç”¨ {len(feature_cols)} å€‹ç‰¹å¾µé€²è¡Œè¨“ç·´ã€‚")
    
    train, test_last_shot, original_max_label = preprocess(train, test) # (*** ä¿®æ”¹ ***) original_max_label ç¾åœ¨æœƒè¢«æ­£ç¢ºè¨­å®š

    # --- 4. å»ºç«‹ N -> N+1 è¨“ç·´è³‡æ–™ ---
    X, y, rally_uids_for_split = create_training_data(train, feature_cols) # (*** ä¿®æ”¹ ***) y ç¾åœ¨æ˜¯ pointId
    X_test = test_last_shot[feature_cols].copy().fillna(0)

    # --- 5. å»ºç«‹ Group Split ---
    X_train, X_valid, y_train, y_valid = create_group_split(X, y, rally_uids_for_split)
    num_class = y.nunique() # (*** ä¿®æ”¹ ***) é€™æ˜¯ pointId çš„é¡åˆ¥æ•¸é‡
    print(f"âœ… åµæ¸¬åˆ° {num_class} å€‹ pointId é¡åˆ¥ã€‚")

    # --- 6. ç‰¹å¾µé¸å– ---
    print(f"ğŸ§© ç‚º pointId é¸å–å‰ {K_FEATURES} å€‹ç‰¹å¾µ...") # (*** ä¿®æ”¹ ***)
    # æ³¨æ„ï¼šç‰¹å¾µé¸å–åœ¨ X_train ä¸Šé€²è¡Œï¼Œä»¥é¿å…éæ“¬åˆ
    # (*** ä¿®æ”¹ ***) objective ä»ç„¶æ˜¯ multi:softmaxï¼Œå› ç‚º pointId ä¹Ÿæ˜¯å¤šåˆ†é¡
    top_features = select_features(X_train, y_train, 
                                     objective="multi:softmax", 
                                     num_class=num_class, 
                                     top_k=K_FEATURES)
    print(f"ğŸ”¥ pointId Top 5: {top_features[:5]}") # (*** ä¿®æ”¹ ***)
    
    X_train_fs = X_train[top_features]
    X_valid_fs = X_valid[top_features]
    X_test_fs = X_test[top_features]

    # =========================================================
    # â­ 7. è¨­å®šè¶…åƒæ•¸æœå°‹ (RandomizedSearchCV)
    # =========================================================
    print("\nğŸš€ è¨­å®šè¶…åƒæ•¸æœå°‹ (RandomizedSearchCV)...")

    # 7a. å°‡è¨“ç·´é›†å’Œé©—è­‰é›†åˆä½µï¼Œä»¥ç¬¦åˆ PredefinedSplit çš„è¦æ±‚
    X_search = pd.concat([X_train_fs, X_valid_fs])
    y_search = pd.concat([y_train, y_valid])

    # 7b. å»ºç«‹ PredefinedSplit
    # -1 ä»£è¡¨è¨“ç·´é›†, 0 ä»£è¡¨é©—è­‰é›†
    test_fold = np.zeros(len(X_search))
    test_fold[:len(X_train_fs)] = -1
    ps = PredefinedSplit(test_fold)

    # 7c. ç‚ºæœå°‹è³‡æ–™è¨ˆç®— 'balanced' æ¬Šé‡
    print("   > æ­£åœ¨è¨ˆç®— 'balanced' é¡åˆ¥æ¬Šé‡ (for Search)...")
    search_weights = compute_sample_weight(
        class_weight='balanced',
        y=y_search
    )
    
    # 7d. ç‚º early stopping æº–å‚™ fit_params
    fit_params = {
        "eval_set": [(X_valid_fs, y_valid)],
        "verbose": False
    }

    # æª¢æŸ¥ XGBoost ç‰ˆæœ¬æ˜¯å¦æ”¯æ´ eval_sample_weight
    if xgb.__version__ >= "2.0.0":
        print("   > åµæ¸¬åˆ° XGBoost >= 2.0.0ï¼Œå•Ÿç”¨ eval_sample_weightã€‚")
        valid_weights = compute_sample_weight(class_weight='balanced', y=y_valid)
        fit_params["sample_weight_eval_set"] = [valid_weights]
    else:
        print(f"   > è­¦å‘Š: XGBoost ç‰ˆæœ¬ ({xgb.__version__}) éèˆŠï¼Œç„¡æ³•ä½¿ç”¨ eval_sample_weightã€‚")

    # 7e. å®šç¾©åƒæ•¸ç¶²æ ¼ (param_distributions)
    param_dist = {
        'learning_rate': [0.05, 0.1, 0.15, 0.2],
        'max_depth': [3, 5, 7, 9],
        'n_estimators': [100, 200, 300, 400],
        'subsample': [0.7, 0.8, 0.9],
        'colsample_bytree': [0.7, 0.8, 0.9],
        'gamma': [0, 0.1, 0.2]
    }

    # 7f. å»ºç«‹åŸºæœ¬æ¨¡å‹
    base_model = xgb.XGBClassifier(
        objective="multi:softmax",
        eval_metric="mlogloss",
        random_state=42,
        tree_method="hist",
        num_class=num_class, # (*** ä¿®æ”¹ ***)
        early_stopping_rounds=30 
    )

    # 7g. å»ºç«‹ RandomizedSearchCV ç‰©ä»¶
    rand_search = RandomizedSearchCV(
        estimator=base_model,
        param_distributions=param_dist,
        n_iter=N_ITER_SEARCH,  # æœå°‹æ¬¡æ•¸
        scoring='f1_macro',    # æˆ‘å€‘çš„ç›®æ¨™æŒ‡æ¨™
        cv=ps,                 # ä½¿ç”¨æˆ‘å€‘è‡ªè¨‚çš„ (Train/Valid) åˆ‡åˆ†
        n_jobs=-1,             # ä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒ
        verbose=2,             # é¡¯ç¤ºæœå°‹é€²åº¦
        random_state=42
    )

    # =========================================================
    # â­ 8. åŸ·è¡Œæœå°‹èˆ‡è©•ä¼°
    # =========================================================
    print("\nğŸš€ é–‹å§‹åŸ·è¡Œè¶…åƒæ•¸æœå°‹...")
    
    # å°‡ search_weights å‚³éçµ¦ fit
    rand_search.fit(
        X_search, 
        y_search, 
        sample_weight=search_weights,
        **fit_params
    )

    print("\nğŸ“Š æœå°‹å®Œæˆ!")
    print(f"âœ… æœ€ä½³åƒæ•¸: {rand_search.best_params_}")
    print(f"âœ… æœ€ä½³ F1 Macro (Val): {rand_search.best_score_:.4f}")

    # å–å¾—æœ€ä½³æ¨¡å‹
    model = rand_search.best_estimator_
    y_pred = model.predict(X_valid_fs)
    plot_and_print_confusion_matrix(y_valid, y_pred, "pointId (é©—è­‰é›†) æ··æ·†çŸ©é™£")

    # =========================================================
    # 9. ç”¢ç”Ÿé æ¸¬ (*** ä¿®æ”¹ ***)
    # =========================================================
    print("\nğŸ§® ç”¢ç”Ÿæ¸¬è©¦é æ¸¬ä¸­...")
    pred_test = model.predict(X_test_fs)
    # (*** æ¢å¾© ***) 
    pred_test_reverted = revert_negative(pred_test, original_max_label)
    
    # å„²å­˜ (*** ä¿®æ”¹ ***)
    save_submission(test_last_shot, pred_test_reverted, SAMPLE_SUB_PATH, SUBMISSION_PATH)
    
    print("\nğŸ‰ æµç¨‹åŸ·è¡Œå®Œç•¢ã€‚")
    
# ä½ å¯ä»¥åœ¨é©—è­‰éšæ®µé€™æ¨£å‘¼å«ï¼š
# plot_and_print_confusion_matrix(y_valid, model.predict(X_valid), "pointId (é©—è­‰é›†) æ··æ·†çŸ©é™£")

if __name__ == "__main__":
    main()

