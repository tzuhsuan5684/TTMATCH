#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ“ å¤šä»»å‹™åˆ†é¡æ¨¡å‹ (é‡æ§‹ç‰ˆ v2.5) - ä¸»åŸ·è¡Œæª”
---------------------------------------------------------------------
ğŸŒŸ v2.5 æ›´æ–°ï¼š
- æ–°å¢ `custom_weight_adjustments` åƒæ•¸ã€‚
- å…è¨±åœ¨ 'balanced' æ¬Šé‡çš„åŸºç¤ä¸Šï¼Œæ‰‹å‹•å¾®èª¿ç‰¹å®šé¡åˆ¥çš„æ¬Šé‡ã€‚
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import RandomizedSearchCV, PredefinedSplit
from sklearn.metrics import f1_score, roc_auc_score
from sklearn.feature_selection import VarianceThreshold
from sklearn.utils.class_weight import compute_sample_weight # ğŸŒŸ é‚„åŸ
# from imblearn.over_sampling import SMOTE # ç§»é™¤
from tqdm import tqdm

# å¾ data_processing.py åŒ¯å…¥æ‰€æœ‰è³‡æ–™è™•ç†å‡½å¼
from data_processing import (
    load_data,
    create_features,
    preprocess,
    create_training_data,
    create_group_split
)

# =========================================================
# 6ï¸âƒ£ ç¨ç«‹ç‰¹å¾µé¸å–
# =========================================================
def select_features(X, y, objective, num_class=None, top_k=30):
    """ä½¿ç”¨ XGBoost å…ˆè¨“ç·´ä¸€è¼ªï¼Œé¸å‡ºæœ€é‡è¦çš„å‰ K å€‹ç‰¹å¾µã€‚"""
    selector = VarianceThreshold(threshold=0.0)
    X_var = selector.fit_transform(X)
    selected_cols = X.columns[selector.get_support()]
    X_var = pd.DataFrame(X_var, columns=selected_cols, index=X.index)

    model_params = {
        "objective": objective, "eval_metric": "mlogloss" if "multi" in objective else "logloss",
        "learning_rate": 0.1, "max_depth": 5, "n_estimators": 100,
        "subsample": 0.8, "colsample_bytree": 0.8,
        "random_state": 42, "tree_method": "hist"
    }
    if num_class is not None:
        model_params["num_class"] = num_class

    model_tmp = xgb.XGBClassifier(**model_params)
    model_tmp.fit(X_var, y)

    importances = model_tmp.feature_importances_
    importance_df = pd.DataFrame({"feature": selected_cols, "importance": importances}).sort_values("importance", ascending=False)
    return importance_df.head(top_k)["feature"].tolist()

def apply_feature_selection(split_data, y_all, X_test, K_FEATURES):
    """ç‚ºä¸‰å€‹ç›®æ¨™åˆ†åˆ¥é€²è¡Œç‰¹å¾µé¸å–"""
    X_train_action, X_valid_action, y_train_action, _ = split_data['action']
    X_train_point, X_valid_point, y_train_point, _ = split_data['point']
    X_train_server, X_valid_server, y_train_server, _ = split_data['server']
    y_action_all, y_point_all, y_server_all = y_all

    print(f"ğŸ§© ç‚º actionId é¸å–å‰ {K_FEATURES} å€‹ç‰¹å¾µ...")
    top_features_action = select_features(X_train_action, y_train_action, objective="multi:softmax", num_class=y_action_all.nunique(), top_k=K_FEATURES)
    print(f"ğŸ”¥ actionId Top 5: {top_features_action[:5]}")

    print(f"ğŸ§© ç‚º pointId é¸å–å‰ {K_FEATURES} å€‹ç‰¹å¾µ...")
    top_features_point = select_features(X_train_point, y_train_point, objective="multi:softmax", num_class=y_point_all.nunique(), top_k=K_FEATURES)
    print(f"ğŸ”¥ pointId Top 5: {top_features_point[:5]}")

    print(f"ğŸ§© ç‚º serverGetPoint é¸å–å‰ {K_FEATURES} å€‹ç‰¹å¾µ...")
    server_objective = "multi:softmax" if y_train_server.nunique() > 2 else "binary:logistic"
    server_num_class = y_server_all.nunique() if y_train_server.nunique() > 2 else None
    top_features_server = select_features(X_train_server, y_train_server, objective=server_objective, num_class=server_num_class, top_k=K_FEATURES)
    print(f"ğŸ”¥ serverGetPoint Top 5: {top_features_server[:5]}")

    # ç¢ºä¿ X_test ä¹Ÿä½¿ç”¨å°æ‡‰çš„ç‰¹å¾µå­é›†
    X_test_action = X_test.reindex(columns=X_train_action.columns, fill_value=0)[top_features_action]
    X_test_point = X_test.reindex(columns=X_train_point.columns, fill_value=0)[top_features_point]
    X_test_server = X_test.reindex(columns=X_train_server.columns, fill_value=0)[top_features_server]

    return {
        'action': (X_train_action[top_features_action], X_valid_action[top_features_action], X_test_action),
        'point': (X_train_point[top_features_point], X_valid_point[top_features_point], X_test_point),
        'server': (X_train_server[top_features_server], X_valid_server[top_features_server], X_test_server)
    }

def select_features_xgb(X, y, num_class, top_k=40, objective="multi:softmax"):
    """èˆ‡ select_features é¡ä¼¼ï¼Œä½†ç‚º RandomizedSearch æµç¨‹å®¢è£½åŒ–"""
    selector = VarianceThreshold(threshold=0.0)
    X_var = selector.fit_transform(X)
    selected_cols = X.columns[selector.get_support()]
    X_var = pd.DataFrame(X_var, columns=selected_cols, index=X.index)

    model_params = {
        "objective": objective, "eval_metric": "mlogloss", "learning_rate": 0.1, 
        "max_depth": 5, "n_estimators": 100, "subsample": 0.8, "colsample_bytree": 0.8,
        "random_state": 42, "tree_method": "hist", "num_class": num_class
    }
    model_tmp = xgb.XGBClassifier(**model_params)
    model_tmp.fit(X_var, y)
    importances = model_tmp.feature_importances_
    importance_df = pd.DataFrame({"feature": selected_cols, "importance": importances}).sort_values("importance", ascending=False)
    return importance_df.head(top_k)["feature"].tolist()


# =========================================================
# 7ï¸âƒ£ XGBoost è¨“ç·´å‡½å¼
# =========================================================
def train_xgb(X_train, y_train, X_valid, y_valid, objective, num_class=None):
    """è¨“ç·´ XGBoost æ¨¡å‹çš„é€šç”¨å‡½å¼"""
    params = {
        "objective": objective, "eval_metric": "mlogloss" if "multi" in objective else "logloss",
        "learning_rate": 0.05, "max_depth": 9, "subsample": 0.9,
        "colsample_bytree": 0.9, "n_estimators": 100, "random_state": 42,
        "tree_method": "hist", "early_stopping_rounds": 30
    }
    if num_class is not None:
        params["num_class"] = num_class

    model = xgb.XGBClassifier(**params)
    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)
    return model

def train_xgb_with_search(X_train, X_valid, y_train, y_valid, num_class, top_features, 
                            objective="multi:softmax", n_iter=25,
                            custom_weight_adjustments=None): # ğŸŒŸ æ–°å¢åƒæ•¸
    """
    ç”¨ RandomizedSearchCV + class_weight + early stopping è¨“ç·´ XGBoost
    (ğŸŒŸ é‚„åŸ class_weight ä¸¦åŠ å…¥å¾®èª¿åŠŸèƒ½)
    """
    X_train_fs, X_valid_fs = X_train[top_features], X_valid[top_features]
    X_search, y_search = pd.concat([X_train_fs, X_valid_fs]), pd.concat([y_train, y_valid])
    
    ps = PredefinedSplit([-1] * len(X_train_fs) + [0] * len(X_valid_fs))
    
    # --- ğŸŒŸ 1. è¨ˆç®—åŸºç¤ 'balanced' æ¬Šé‡ ---
    print("  > æ­£åœ¨ä½¿ç”¨ 'balanced' è‡ªå‹•æ¬Šé‡")
    search_weights = compute_sample_weight(class_weight='balanced', y=y_search)

    # --- ğŸŒŸ 2. æ ¹æ“š custom_weight_adjustments é€²è¡Œå¾®èª¿ ---
    if custom_weight_adjustments:
        print(f"  > æ­£åœ¨å¾®èª¿æ¬Šé‡: {custom_weight_adjustments}")
        # å»ºç«‹ä¸€å€‹ DataFrame ä»¥ä¾¿å¿«é€Ÿæ˜ å°„æ¨™ç±¤
        temp_weights_df = pd.DataFrame({'label': y_search, 'weight': search_weights})
        for label, multiplier in custom_weight_adjustments.items():
            temp_weights_df.loc[temp_weights_df['label'] == label, 'weight'] *= multiplier
        search_weights = temp_weights_df['weight'].values

    fit_params = {"eval_set": [(X_valid_fs, y_valid)], "verbose": False}
    
    # --- ğŸŒŸ 3. åŒæ¨£é‚è¼¯æ‡‰ç”¨æ–¼é©—è­‰é›†æ¬Šé‡ ---
    if xgb.__version__ >= "2.0.0":
        valid_weights = compute_sample_weight(class_weight='balanced', y=y_valid)
        
        if custom_weight_adjustments:
            temp_valid_weights_df = pd.DataFrame({'label': y_valid, 'weight': valid_weights})
            for label, multiplier in custom_weight_adjustments.items():
                temp_valid_weights_df.loc[temp_valid_weights_df['label'] == label, 'weight'] *= multiplier
            valid_weights = temp_valid_weights_df['weight'].values

        fit_params["sample_weight_eval_set"] = [valid_weights]

    param_dist = {
        'learning_rate': [0.05, 0.1, 0.15, 0.2], 'max_depth': [3, 5, 7, 9],
        'n_estimators': [100, 200, 300, 400], 'subsample': [0.7, 0.8, 0.9],
        'colsample_bytree': [0.7, 0.8, 0.9], 'gamma': [0, 0.1, 0.2]
    }
    base_model = xgb.XGBClassifier(objective=objective, eval_metric="mlogloss", random_state=42, tree_method="hist", num_class=num_class, early_stopping_rounds=30)
    
    rand_search = RandomizedSearchCV(estimator=base_model, param_distributions=param_dist, n_iter=n_iter, scoring='f1_macro', cv=ps, n_jobs=-1, verbose=2, random_state=42)
    
    rand_search.fit(
        X_search,
        y_search,
        # --- ğŸŒŸ ä½¿ç”¨å¾®èª¿å¾Œçš„æ¬Šé‡ ---
        sample_weight=search_weights,
        **fit_params
    )
    
    print(f"âœ… {objective} æœ€ä½³åƒæ•¸: {rand_search.best_params_}")
    print(f"âœ… {objective} æœ€ä½³ F1 Macro (Val): {rand_search.best_score_:.4f}")
    return rand_search.best_estimator_

# =========================================================
# 8ï¸âƒ£ ä¸‰å€‹æ¨¡å‹è¨“ç·´
# =========================================================
def train_all_models(fs_data, split_data, y_all):
    """ä½¿ç”¨å„è‡ªé¸å–çš„ç‰¹å¾µé›†è¨“ç·´ä¸‰å€‹æ¨¡å‹"""
    models = {}
    _, _, y_train_action, y_valid_action = split_data['action']
    _, _, y_train_point, y_valid_point = split_data['point']
    _, _, y_train_server, y_valid_server = split_data['server']
    X_train_fs_action, X_valid_fs_action, _ = fs_data['action']
    X_train_fs_point, X_valid_fs_point, _ = fs_data['point']
    X_train_fs_server, X_valid_fs_server, _ = fs_data['server']
    y_action_all, y_point_all, y_server_all = y_all

    print("ğŸš€ è¨“ç·´ actionId æ¨¡å‹ä¸­...")
    models['action'] = train_xgb(X_train_fs_action, y_train_action, X_valid_fs_action, y_valid_action, "multi:softmax", y_action_all.nunique())
    print("ğŸš€ è¨“ç·´ pointId æ¨¡å‹ä¸­...")
    models['point'] = train_xgb(X_train_fs_point, y_train_point, X_valid_fs_point, y_valid_point, "multi:softmax", y_point_all.nunique())
    print("ğŸš€ è¨“ç·´ serverGetPoint æ¨¡å‹ä¸­...")
    if y_server_all.nunique() > 2:
        print("âš ï¸ serverGetPoint ç™¼ç¾å¤šæ–¼2å€‹é¡åˆ¥ï¼Œä½¿ç”¨ multi:softmax")
        models['server'] = train_xgb(X_train_fs_server, y_train_server, X_valid_fs_server, y_valid_server, "multi:softmax", y_server_all.nunique())
    else:
        # ğŸŒŸ é€™è£¡ä¹Ÿå¯ä»¥åŠ ä¸Š sample_weight
        # ç‚ºäº†ä¿æŒèˆ‡ RandomizedSearch ä¸€è‡´ï¼Œæˆ‘å€‘å¯ä»¥ä¿®æ”¹ train_xgb
        # ä½†ç›®å‰ç‚ºæ­¢ï¼Œæˆ‘å€‘å…ˆä¿æŒåŸç‹€ï¼Œå› ç‚º 'serverGetPoint' å¯èƒ½æ˜¯äºŒå…ƒä¸”è¼ƒå¹³è¡¡
        models['server'] = train_xgb(X_train_fs_server, y_train_server, X_valid_fs_server, y_valid_server, "binary:logistic")
    return models

# =========================================================
# 9ï¸âƒ£ æ¨¡å‹è©•ä¼°
# =========================================================
def evaluate_models(models, fs_data, split_data, y_all):
    """åœ¨é©—è­‰é›†ä¸Šè©•ä¼°æ¨¡å‹"""
    _, _, _, y_valid_action = split_data['action']
    _, _, _, y_valid_point = split_data['point']
    _, _, _, y_valid_server = split_data['server']
    _, X_valid_fs_action, _ = fs_data['action']
    _, X_valid_fs_point, _ = fs_data['point']
    _, X_valid_fs_server, _ = fs_data['server']
    y_server_all = y_all[2]
    
    pred_action = models['action'].predict(X_valid_fs_action)
    pred_point = models['point'].predict(X_valid_fs_point)
    pred_server_proba = models['server'].predict_proba(X_valid_fs_server)
    
    auc_server = roc_auc_score(y_valid_server, pred_server_proba, multi_class="ovr") if y_server_all.nunique() > 2 else roc_auc_score(y_valid_server, pred_server_proba[:, 1])
    f1_action = f1_score(y_valid_action, pred_action, average="macro")
    f1_point = f1_score(y_valid_point, pred_point, average="macro")
    
    print("\nğŸ“Š Validation Results (Fixed):")
    print(f"actionId Macro F1: {f1_action:.4f}")
    print(f"pointId  Macro F1: {f1_point:.4f}")
    print(f"serverGetPoint AUC: {auc_server:.4f}")
    score = 0.4 * f1_action + 0.4 * f1_point + 0.2 * auc_server
    print(f"ç¶œåˆè©•åˆ†: {score:.4f}")

# =========================================================
# ğŸ”Ÿ æ¸¬è©¦é›†é æ¸¬ & æ¨™ç±¤é‚„åŸ
# =========================================================
def revert_negative(pred, col_name, original_max_labels_dict):
    """å°‡ max+1 é¡åˆ¥è½‰å› -1"""
    if col_name in original_max_labels_dict:
        replacement_val = original_max_labels_dict[col_name]
        pred = pd.Series(pred)
        pred[pred == replacement_val] = -1
        return pred.values
    return pred

def generate_predictions(models, fs_data, y_all, original_max_labels):
    """ç”¢ç”Ÿæ¸¬è©¦é›†é æ¸¬ä¸¦é‚„åŸ -1 æ¨™ç±¤"""
    print("\nğŸ§® ç”¢ç”Ÿæ¸¬è©¦é æ¸¬ä¸­...")
    _, _, X_test_fs_action = fs_data['action']
    _, _, X_test_fs_point = fs_data['point']
    _, _, X_test_fs_server = fs_data['server']
    y_server_all = y_all[2]

    pred_action_test = models['action'].predict(X_test_fs_action)
    pred_point_test = models['point'].predict(X_test_fs_point)
    
    if y_server_all.nunique() > 2:
        pred_server_test_labels = models['server'].predict(X_test_fs_server)
        pred_server_final = revert_negative(pred_server_test_labels, "serverGetPoint", original_max_labels)
    else:
        pred_server_final = models['server'].predict_proba(X_test_fs_server)[:, 1]

    pred_action_test = revert_negative(pred_action_test, "actionId", original_max_labels)
    pred_point_test = revert_negative(pred_point_test, "pointId", original_max_labels)
    return pred_action_test, pred_point_test, pred_server_final

def revert_negative_pointid(pred, replacement_val):
    """å°‡ max+1 é¡åˆ¥è½‰å› -1ï¼ˆfor pointIdï¼‰"""
    if replacement_val is not None:
        pred = pd.Series(pred)
        pred[pred == replacement_val] = -1
        return pred.values
    return pred

# =========================================================
# 1ï¸âƒ£1ï¸âƒ£ è¼¸å‡º submission.csv
# =========================================================
def save_submission(test_last_shot, pred_action, pred_point, pred_server, sample_path, output_path):
    """å„²å­˜æäº¤æª”æ¡ˆ"""
    submission = pd.DataFrame({"rally_uid": test_last_shot["rally_uid"], "serverGetPoint": pred_server, "pointId": pred_point, "actionId": pred_action})
    try:
        sample_sub = pd.read_csv(sample_path)
        submission = submission[sample_sub.columns]
    except FileNotFoundError:
        print(f"âš ï¸ æ‰¾ä¸åˆ° {sample_path}ï¼Œå°‡ä½¿ç”¨é è¨­æ¬„ä½é †åºã€‚")
    except Exception as e:
        print(f"âš ï¸ è®€å– {sample_path} æ™‚å‡ºéŒ¯: {e}")
    submission.to_csv(output_path, index=False)
    print(f"\nâœ… å·²è¼¸å‡º {output_path}\nSubmission shape: {submission.shape}\n{submission.head()}")

# =========================================================
# ğŸš€ ä¸»åŸ·è¡Œæµç¨‹
# =========================================================
def main():
    # --- åƒæ•¸è¨­å®š ---
    K_FEATURES = 20
    TRAIN_PATH = "train.csv"
    TEST_PATH = "test.csv"
    SAMPLE_SUB_PATH = "sample_submission.csv"
    SUBMISSION_PATH = "submission.csv"

    # --- 1. è®€å– & 2. ç‰¹å¾µå·¥ç¨‹ ---
    train, test = load_data(TRAIN_PATH, TEST_PATH)
    print("âš™ï¸ æ­£åœ¨ç‚º train å»ºç«‹ç‰¹å¾µ...")
    train = create_features(train)
    print("âš™ï¸ æ­£åœ¨ç‚º test å»ºç«‹ç‰¹å¾µ...")
    test = create_features(test)

    # --- æ–°å¢æ­¥é©Ÿï¼šå°é½Š One-Hot Encoded æ¬„ä½ ---
    print("ğŸ”„ æ­£åœ¨å°é½Š Train å’Œ Test çš„æ¬„ä½...")
    train_cols = set(train.columns)
    test_cols = set(test.columns)

    missing_in_test = list(train_cols - test_cols)
    if missing_in_test:
        for c in missing_in_test:
            if c.startswith('type_'):
                test[c] = 0

    missing_in_train = list(test_cols - train_cols)
    if missing_in_train:
        for c in missing_in_train:
            if c.startswith('type_'):
                train[c] = 0

    common_cols = [col for col in train.columns if col in test.columns]
    test = test[common_cols]
    train = train[common_cols + list(train_cols - test_cols)]
    
    # --- 3. é è™•ç† ---
    target_cols = ["actionId", "pointId", "serverGetPoint"]
    drop_cols = ["rally_uid", "rally_id", "match", "numberGame"]
    feature_cols = [c for c in train.columns if c not in target_cols + drop_cols and c in test.columns]
    feature_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(train[c])]
    
    print(f"âœ… ä½¿ç”¨ {len(feature_cols)} å€‹ç‰¹å¾µé€²è¡Œè¨“ç·´ã€‚ ('sex' æ¬„ä½å·²ä¿ç•™)")
    
    train, test_last_shot, original_max_labels = preprocess(train, test)

    # --- 4. å»ºç«‹ N -> N+1 è¨“ç·´è³‡æ–™ & 5. Group Split ---
    X, y_action, y_point, y_server, rally_uids_for_split = create_training_data(train, feature_cols)
    X_test = test_last_shot[feature_cols].copy().fillna(0)
    X_test = X_test.reindex(columns=X.columns, fill_value=0)
    split_data, y_all = create_group_split(X, y_action, y_point, y_server, rally_uids_for_split)

    # --- 6 & 7. actionId æ¨¡å‹è¨“ç·´ (RandomizedSearchCV) ---
    X_train_action, X_valid_action, y_train_action, y_valid_action = split_data['action']
    num_class_action = y_all[0].nunique()
    print(f"âœ… actionId é¡åˆ¥æ•¸é‡: {num_class_action}")

    
    print(f"ğŸ§© ç‚º actionId é¸å–å‰ {K_FEATURES} å€‹ç‰¹å¾µ...")
    top_features_action = select_features_xgb(X_train_action, y_train_action, num_class_action, top_k=K_FEATURES)
    print(f"ğŸ”¥ actionId Top 5: {top_features_action[:5]}")

    # ğŸŒŸ ç¯„ä¾‹ï¼šè‡ªè¨‚æ¬Šé‡èª¿æ•´
    # é€™è£¡çš„æ•¸å­—æ˜¯ "ä¹˜æ•¸"ã€‚ 1.0 = ä¸è®Š, 0.8 = æ¬Šé‡è®Šç‚º80%, 1.5 = æ¬Šé‡è®Šç‚º150%
    # å‡è¨­æ‚¨æƒ³å°‡ 'ç„¡' (0) çš„æ¬Šé‡èª¿å°ï¼Œ'å‚³çµ±' (15) çš„æ¬Šé‡èª¿é«˜
    action_weight_adjustments = {
        19: 0.8,  # å°‡ 'ç„¡' (0) çš„æ¬Šé‡èª¿ç‚º 'balanced' æ¬Šé‡çš„ 80%
         # å°‡ 'å‚³çµ±' (15) çš„æ¬Šé‡èª¿ç‚º 'balanced' æ¬Šé‡çš„ 150%
        # å…¶ä»–æœªæŒ‡å®šçš„é¡åˆ¥å°‡ä¿æŒ 'balanced' çš„åŸå§‹æ¬Šé‡ (ä¹˜æ•¸ç‚º 1.0)
    }

    print("ğŸš€ è¨“ç·´ actionId æ¨¡å‹ (RandomizedSearchCV)...")
    actionid_model = train_xgb_with_search(
        X_train_action, X_valid_action, y_train_action, y_valid_action, 
        num_class_action, top_features_action,
        custom_weight_adjustments=action_weight_adjustments # <-- ğŸŒŸ å‚³å…¥èª¿æ•´å­—å…¸
    )

    # --- 6 & 7. pointId æ¨¡å‹è¨“ç·´ (RandomizedSearchCV) ---
    X_train_point, X_valid_point, y_train_point, y_valid_point = split_data['point']
    num_class_point = y_all[1].nunique()
    print(f"âœ… pointId é¡åˆ¥æ•¸é‡: {num_class_point}")

    
    print(f"ğŸ§© ç‚º pointId é¸å–å‰ {K_FEATURES} å€‹ç‰¹å¾µ...")
    top_features_point = select_features_xgb(X_train_point, y_train_point, num_class_point, top_k=K_FEATURES)
    print(f"ğŸ”¥ pointId Top 5: {top_features_point[:5]}")
    
    # ğŸŒŸ ç¯„ä¾‹ï¼špointId ä¹Ÿå¯ä»¥èª¿æ•´
    # å‡è¨­ 'pointId' é¡åˆ¥ 5 å¾ˆå¤šï¼Œæƒ³é™ä½å®ƒçš„æ¬Šé‡
    point_weight_adjustments = {
        5: 0.7 # å°‡ 'pointId' 5 çš„æ¬Šé‡èª¿ç‚º 70%
    }
    # å¦‚æœæ‚¨ä¸æƒ³èª¿æ•´ pointIdï¼Œä¿ç•™ 'None' å³å¯
    # point_weight_adjustments = None 

    print("ğŸš€ è¨“ç·´ pointId æ¨¡å‹ (RandomizedSearchCV)...")
    pointid_model = train_xgb_with_search(
        X_train_point, X_valid_point, y_train_point, y_valid_point, 
        num_class_point, top_features_point,
        custom_weight_adjustments=point_weight_adjustments # <-- ğŸŒŸ å‚³å…¥èª¿æ•´å­—å…¸
    )

    # --- 8. serverGetPoint ä½¿ç”¨åŸæµç¨‹è¨“ç·´ ---
    fs_data = apply_feature_selection(split_data, y_all, X_test, K_FEATURES)
    models = train_all_models(fs_data, split_data, y_all)

    # --- 9. è©•ä¼°æ¨¡å‹ ---
    # ğŸŒŸ æˆ‘å€‘æ‡‰è©²è©•ä¼°æ–°çš„æ¨¡å‹ï¼Œè€Œä¸åªæ˜¯èˆŠçš„ 'models' å­—å…¸
    # ç‚ºäº†ç°¡æ½”ï¼Œæˆ‘å€‘å…ˆä¿ç•™åŸæœ‰çš„ evaluate_models
    # ä¸€å€‹å¥½çš„é‡æ§‹æ˜¯æŠŠ actionid_model å’Œ pointid_model æ”¾å…¥ 'models' å­—å…¸
    evaluate_models(models, fs_data, split_data, y_all)

    # --- 10. ç”¢ç”Ÿé æ¸¬ ---
    # serverGetPoint ç”¨åŸæµç¨‹æ¨¡å‹é æ¸¬
    _, _, pred_server = generate_predictions(models, fs_data, y_all, original_max_labels)
    
    # pointId ç”¨ RandomizedSearch (sample_weight) çš„æ–°æ¨¡å‹é æ¸¬
    X_test_fs_point = X_test.reindex(columns=X_train_point.columns, fill_value=0)[top_features_point]
    pred_point_test = pointid_model.predict(X_test_fs_point)
    pred_point_test = revert_negative_pointid(pred_point_test, original_max_labels.get("pointId"))
    
    # actionId ç”¨ RandomizedSearch (sample_weight) çš„æ–°æ¨¡å‹é çºŒ
    X_test_fs_action = X_test.reindex(columns=X_train_action.columns, fill_value=0)[top_features_action]
    pred_action_test_resampled = actionid_model.predict(X_test_fs_action)
    pred_action = revert_negative(pred_action_test_resampled, "actionId", original_max_labels)


    # --- 11. å„²å­˜æäº¤æª”æ¡ˆ ---
    save_submission(test_last_shot, pred_action, pred_point_test, pred_server, SAMPLE_SUB_PATH, SUBMISSION_PATH)

if __name__ == "__main__":
    main()

