#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ“ å¤šä»»å‹™åˆ†é¡æ¨¡å‹ (é‡æ§‹ç‰ˆ v2.7) - ä¸»åŸ·è¡Œæª”
---------------------------------------------------------------------
ğŸŒŸ v2.7 æ›´æ–°ï¼š
- æ–°å¢ `plot_confusion_matrix` å‡½å¼ï¼Œç”¨æ–¼è©•ä¼°å¾Œç¹ªè£½ä¸¦å„²å­˜æ··æ·†çŸ©é™£åœ–ã€‚
- åœ¨ä¸»æµç¨‹ä¸­åŠ å…¥å° actionId å’Œ pointId çš„æ··æ·†çŸ©é™£ç”Ÿæˆã€‚
- åŒ¯å…¥ `matplotlib.pyplot`ã€`seaborn` å’Œ `sklearn.metrics.confusion_matrix`ã€‚

ğŸŒŸ v2.6 æ›´æ–°ï¼š
- æ–°å¢ `log_experiment_results` å‡½å¼ï¼Œå°‡æ¯æ¬¡é‹è¡Œçš„åˆ†æ•¸å’Œåƒæ•¸è¨˜éŒ„åˆ° 'experiment_log.csv'ã€‚
- ä¿®æ­£ `main()` å‡½å¼ä¸­çš„é‚è¼¯ï¼Œç¢ºä¿ã€Œè©•ä¼°ã€å’Œã€Œé æ¸¬ã€ä½¿ç”¨çš„æ˜¯åŒä¸€çµ„æ¨¡å‹ã€‚
- ç§»é™¤ `evaluate_models`ã€`generate_predictions`ã€`train_all_models`ã€`apply_feature_selection`
  ç­‰å‡½å¼ï¼Œå°‡å…¶ç°¡åŒ–ä¸¦æ•´åˆåˆ° `main()` æµç¨‹ä¸­ï¼Œä»¥ç¢ºä¿é‚è¼¯ä¸€è‡´æ€§ã€‚
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import RandomizedSearchCV, PredefinedSplit
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix # ğŸŒŸ æ–°å¢ confusion_matrix
from sklearn.feature_selection import VarianceThreshold
from sklearn.utils.class_weight import compute_sample_weight
from tqdm import tqdm
import csv
import os
from datetime import datetime
import json
import matplotlib.pyplot as plt # ğŸŒŸ æ–°å¢
import seaborn as sns           # ğŸŒŸ æ–°å¢


# å¾ data_processing.py åŒ¯å…¥æ‰€æœ‰è³‡æ–™è™•ç†å‡½å¼
from data_processing import (
    load_data,
    create_features,
    preprocess,
    create_training_data,
    create_group_split
)

# =========================================================
# 6ï¸âƒ£ ç¨ç«‹ç‰¹å¾µé¸å–
# =========================================================
def select_features(X, y, objective, num_class=None, top_k=30):
    """ä½¿ç”¨ XGBoost å…ˆè¨“ç·´ä¸€è¼ªï¼Œé¸å‡ºæœ€é‡è¦çš„å‰ K å€‹ç‰¹å¾µã€‚"""
    selector = VarianceThreshold(threshold=0.0)
    X_var = selector.fit_transform(X)
    selected_cols = X.columns[selector.get_support()]
    X_var = pd.DataFrame(X_var, columns=selected_cols, index=X.index)

    model_params = {
        "objective": objective, "eval_metric": "mlogloss" if "multi" in objective else "logloss",
        "learning_rate": 0.1, "max_depth": 5, "n_estimators": 100,
        "subsample": 0.8, "colsample_bytree": 0.8,
        "random_state": 42, "tree_method": "hist"
    }
    if num_class is not None:
        model_params["num_class"] = num_class

    model_tmp = xgb.XGBClassifier(**model_params)
    model_tmp.fit(X_var, y)

    importances = model_tmp.feature_importances_
    importance_df = pd.DataFrame({"feature": selected_cols, "importance": importances}).sort_values("importance", ascending=False)
    return importance_df.head(top_k)["feature"].tolist()

def select_features_xgb(X, y, num_class, top_k=40, objective="multi:softmax"):
    """èˆ‡ select_features é¡ä¼¼ï¼Œä½†ç‚º RandomizedSearch æµç¨‹å®¢è£½åŒ–"""
    selector = VarianceThreshold(threshold=0.0)
    X_var = selector.fit_transform(X)
    selected_cols = X.columns[selector.get_support()]
    X_var = pd.DataFrame(X_var, columns=selected_cols, index=X.index)

    model_params = {
        "objective": objective, "eval_metric": "mlogloss", "learning_rate": 0.1,
        "max_depth": 5, "n_estimators": 100, "subsample": 0.8, "colsample_bytree": 0.8,
        "random_state": 42, "tree_method": "hist", "num_class": num_class
    }
    model_tmp = xgb.XGBClassifier(**model_params)
    model_tmp.fit(X_var, y)
    importances = model_tmp.feature_importances_
    importance_df = pd.DataFrame({"feature": selected_cols, "importance": importances}).sort_values("importance", ascending=False)
    return importance_df.head(top_k)["feature"].tolist()


# =========================================================
# 7ï¸âƒ£ XGBoost è¨“ç·´å‡½å¼
# =========================================================
def train_xgb(X_train, y_train, X_valid, y_valid, objective, num_class=None):
    """è¨“ç·´ XGBoost æ¨¡å‹çš„é€šç”¨å‡½å¼ï¼ˆç”¨æ–¼ serverGetPointï¼‰"""
    params = {
        "objective": objective, "eval_metric": "mlogloss" if "multi" in objective else "logloss",
        "learning_rate": 0.05, "max_depth": 9, "subsample": 0.9,
        "colsample_bytree": 0.9, "n_estimators": 100, "random_state": 42,
        "tree_method": "hist", "early_stopping_rounds": 30
    }
    if num_class is not None:
        params["num_class"] = num_class

    model = xgb.XGBClassifier(**params)
    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)
    return model

def train_xgb_with_search(X_train, X_valid, y_train, y_valid, num_class, top_features,
                            objective="multi:softmax", n_iter=25,
                            custom_weight_adjustments=None):
    """
    ç”¨ RandomizedSearchCV + class_weight + early stopping è¨“ç·´ XGBoost
    """
    X_train_fs, X_valid_fs = X_train[top_features], X_valid[top_features]
    X_search, y_search = pd.concat([X_train_fs, X_valid_fs]), pd.concat([y_train, y_valid])

    ps = PredefinedSplit([-1] * len(X_train_fs) + [0] * len(X_valid_fs))

    print("  > æ­£åœ¨ä½¿ç”¨ 'balanced' è‡ªå‹•æ¬Šé‡")
    search_weights = compute_sample_weight(class_weight='balanced', y=y_search)

    if custom_weight_adjustments:
        print(f"  > æ­£åœ¨å¾®èª¿æ¬Šé‡: {custom_weight_adjustments}")
        temp_weights_df = pd.DataFrame({'label': y_search, 'weight': search_weights})
        for label, multiplier in custom_weight_adjustments.items():
            temp_weights_df.loc[temp_weights_df['label'] == label, 'weight'] *= multiplier
        search_weights = temp_weights_df['weight'].values

    fit_params = {"eval_set": [(X_valid_fs, y_valid)], "verbose": False}

    if xgb.__version__ >= "2.0.0":
        valid_weights = compute_sample_weight(class_weight='balanced', y=y_valid)

        if custom_weight_adjustments:
            temp_valid_weights_df = pd.DataFrame({'label': y_valid, 'weight': valid_weights})
            for label, multiplier in custom_weight_adjustments.items():
                temp_valid_weights_df.loc[temp_valid_weights_df['label'] == label, 'weight'] *= multiplier
            valid_weights = temp_valid_weights_df['weight'].values

        fit_params["sample_weight_eval_set"] = [valid_weights]

    param_dist = {
        'learning_rate': [0.05, 0.1, 0.15, 0.2], 'max_depth': [3, 5, 7, 9],
        'n_estimators': [100, 200, 300, 400], 'subsample': [0.7, 0.8, 0.9],
        'colsample_bytree': [0.7, 0.8, 0.9], 'gamma': [0, 0.1, 0.2]
    }
    base_model = xgb.XGBClassifier(objective=objective, eval_metric="mlogloss", random_state=42, tree_method="hist", num_class=num_class, early_stopping_rounds=30)

    rand_search = RandomizedSearchCV(estimator=base_model, param_distributions=param_dist, n_iter=n_iter, scoring='f1_macro', cv=ps, n_jobs=-1, verbose=2, random_state=42)

    rand_search.fit(
        X_search,
        y_search,
        sample_weight=search_weights,
        **fit_params
    )

    print(f"âœ… {objective} æœ€ä½³åƒæ•¸: {rand_search.best_params_}")
    print(f"âœ… {objective} æœ€ä½³ F1 Macro (Val): {rand_search.best_score_:.4f}")
    return rand_search.best_estimator_

# =========================================================
# ğŸŒŸ 1ï¸âƒ£0ï¸âƒ£ ç¹ªè£½æ··æ·†çŸ©é™£ (NEW)
# =========================================================
def plot_confusion_matrix(y_true, y_pred, class_labels, title, filename):
    """ç¹ªè£½ä¸¦å„²å­˜æ··æ·†çŸ©é™£åœ–"""
    cm = confusion_matrix(y_true, y_pred, labels=class_labels)
    plt.figure(figsize=(14, 12))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_labels, yticklabels=class_labels)
    plt.title(title, fontsize=16)
    plt.ylabel('True Label', fontsize=12)
    plt.xlabel('Predicted Label', fontsize=12)
    plt.tight_layout()
    plt.savefig(filename)
    plt.close() # é—œé–‰åœ–åƒä»¥é‡‹æ”¾è¨˜æ†¶é«”
    print(f"âœ… å·²å„²å­˜æ··æ·†çŸ©é™£åœ–: {filename}")

# =========================================================
# 1ï¸âƒ£1ï¸âƒ£ è¼¸å‡º submission.csv
# =========================================================
def revert_negative(pred, col_name, original_max_labels_dict):
    """å°‡ max+1 é¡åˆ¥è½‰å› -1"""
    if col_name in original_max_labels_dict:
        replacement_val = original_max_labels_dict[col_name]
        pred = pd.Series(pred)
        pred[pred == replacement_val] = -1
        return pred.values
    return pred

def revert_negative_pointid(pred, replacement_val):
    """å°‡ max+1 é¡åˆ¥è½‰å› -1ï¼ˆfor pointIdï¼‰"""
    if replacement_val is not None:
        pred = pd.Series(pred)
        pred[pred == replacement_val] = -1
        return pred.values
    return pred

def save_submission(test_last_shot, pred_action, pred_point, pred_server, sample_path, output_path):
    """å„²å­˜æäº¤æª”æ¡ˆ"""
    submission = pd.DataFrame({"rally_uid": test_last_shot["rally_uid"], "serverGetPoint": pred_server, "pointId": pred_point, "actionId": pred_action})
    try:
        sample_sub = pd.read_csv(sample_path)
        submission = submission[sample_sub.columns]
    except FileNotFoundError:
        print(f"âš ï¸ æ‰¾ä¸åˆ° {sample_path}ï¼Œå°‡ä½¿ç”¨é è¨­æ¬„ä½é †åºã€‚")
    except Exception as e:
        print(f"âš ï¸ è®€å– {sample_path} æ™‚å‡ºéŒ¯: {e}")
    submission.to_csv(output_path, index=False)
    print(f"\nâœ… å·²è¼¸å‡º {output_path}\nSubmission shape: {submission.shape}\n{submission.head()}")

# =========================================================
# 1ï¸âƒ£2ï¸âƒ£ å¯¦é©—çµæœç´€éŒ„
# =========================================================
def log_experiment_results(log_path, results_dict):
    """å°‡å–®æ¬¡å¯¦é©—çµæœ (å­—å…¸) é™„åŠ åˆ° CSV æª”æ¡ˆä¸­"""
    try:
        loggable_dict = {}
        for key, value in results_dict.items():
            if value is None:
                loggable_dict[key] = "None"
            elif isinstance(value, dict):
                 loggable_dict[key] = json.dumps(value)
            else:
                loggable_dict[key] = value

        fieldnames = loggable_dict.keys()
        file_exists = os.path.isfile(log_path)

        with open(log_path, 'a', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)

            if not file_exists:
                writer.writeheader()

            writer.writerow(loggable_dict)
        print(f"âœ… å¯¦é©—çµæœå·²ç´€éŒ„è‡³ {log_path}")
    except Exception as e:
        print(f"âš ï¸ ç´€éŒ„å¯¦é©—çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

# =========================================================
# ğŸš€ ä¸»åŸ·è¡Œæµç¨‹
# =========================================================
def main():
    # --- åƒæ•¸è¨­å®š ---
    K_FEATURES = 20
    N_ITER_SEARCH = 25
    LOG_FILE = "experiment_log.csv"

    TRAIN_PATH = "train.csv"
    TEST_PATH = "test.csv"
    SAMPLE_SUB_PATH = "sample_submission.csv"
    SUBMISSION_PATH = "submission.csv"

    # --- 1. è®€å– & 2. ç‰¹å¾µå·¥ç¨‹ ---
    train, test = load_data(TRAIN_PATH, TEST_PATH)
    print("âš™ï¸ æ­£åœ¨ç‚º train å»ºç«‹ç‰¹å¾µ...")
    train = create_features(train)
    print("âš™ï¸ æ­£åœ¨ç‚º test å»ºç«‹ç‰¹å¾µ...")
    test = create_features(test)

    # --- æ¬„ä½å°é½Š ---
    print("ğŸ”„ æ­£åœ¨å°é½Š Train å’Œ Test çš„æ¬„ä½...")
    train_cols = set(train.columns)
    test_cols = set(test.columns)
    missing_in_test = list(train_cols - test_cols)
    if missing_in_test:
        for c in missing_in_test:
            if c.startswith('type_'): test[c] = 0
    missing_in_train = list(test_cols - train_cols)
    if missing_in_train:
        for c in missing_in_train:
            if c.startswith('type_'): train[c] = 0
    common_cols = [col for col in train.columns if col in test.columns]
    test = test[common_cols]
    train = train[common_cols + list(train_cols - test_cols)]

    # --- 3. é è™•ç† ---
    target_cols = ["actionId", "pointId", "serverGetPoint"]
    drop_cols = ["rally_uid", "rally_id", "match", "numberGame"]
    feature_cols = [c for c in train.columns if c not in target_cols + drop_cols and c in test.columns]
    feature_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(train[c])]

    print(f"âœ… ä½¿ç”¨ {len(feature_cols)} å€‹ç‰¹å¾µé€²è¡Œè¨“ç·´ã€‚ ('sex' æ¬„ä½å·²ä¿ç•™)")

    train, test_last_shot, original_max_labels = preprocess(train, test)

    # --- 4. å»ºç«‹ N -> N+1 è¨“ç·´è³‡æ–™ & 5. Group Split ---
    X, y_action, y_point, y_server, rally_uids_for_split = create_training_data(train, feature_cols)
    X_test = test_last_shot[feature_cols].copy().fillna(0)
    X_test = X_test.reindex(columns=X.columns, fill_value=0)
    split_data, y_all = create_group_split(X, y_action, y_point, y_server, rally_uids_for_split)
    y_action_all, y_point_all, y_server_all = y_all

    # --- 6 & 7. actionId æ¨¡å‹è¨“ç·´ (RandomizedSearchCV) ---
    X_train_action, X_valid_action, y_train_action, y_valid_action = split_data['action']
    num_class_action = y_action_all.nunique()
    print(f"âœ… actionId é¡åˆ¥æ•¸é‡: {num_class_action}")

    print(f"ğŸ§© ç‚º actionId é¸å–å‰ {K_FEATURES} å€‹ç‰¹å¾µ...")
    top_features_action = select_features_xgb(X_train_action, y_train_action, num_class_action, top_k=K_FEATURES)
    print(f"ğŸ”¥ actionId Top 5: {top_features_action[:5]}")

    action_weight_adjustments = { 19: 1.5}

    print("ğŸš€ è¨“ç·´ actionId æ¨¡å‹ (RandomizedSearchCV)...")
    actionid_model = train_xgb_with_search(
        X_train_action, X_valid_action, y_train_action, y_valid_action,
        num_class_action, top_features_action,
        n_iter=N_ITER_SEARCH,
        custom_weight_adjustments=action_weight_adjustments
    )

    # --- 6 & 7. pointId æ¨¡å‹è¨“ç·´ (RandomizedSearchCV) ---
    X_train_point, X_valid_point, y_train_point, y_valid_point = split_data['point']
    num_class_point = y_point_all.nunique()
    print(f"âœ… pointId é¡åˆ¥æ•¸é‡: {num_class_point}")

    print(f"ğŸ§© ç‚º pointId é¸å–å‰ {K_FEATURES} å€‹ç‰¹å¾µ...")
    top_features_point = select_features_xgb(X_train_point, y_train_point, num_class_point, top_k=K_FEATURES)
    print(f"ğŸ”¥ pointId Top 5: {top_features_point[:5]}")

    point_weight_adjustments = None

    print("ğŸš€ è¨“ç·´ pointId æ¨¡å‹ (RandomizedSearchCV)...")
    pointid_model = train_xgb_with_search(
        X_train_point, X_valid_point, y_train_point, y_valid_point,
        num_class_point, top_features_point,
        n_iter=N_ITER_SEARCH,
        custom_weight_adjustments=point_weight_adjustments
    )

    # --- 8. åƒ…è¨“ç·´ serverGetPoint æ¨¡å‹ ---
    print("ğŸš€ è¨“ç·´ serverGetPoint æ¨¡å‹ä¸­...")
    X_train_server, X_valid_server, y_train_server, y_valid_server = split_data['server']

    server_objective = "binary:logistic" if y_server_all.nunique() <= 2 else "multi:softmax"
    server_num_class = y_server_all.nunique() if y_server_all.nunique() > 2 else None

    top_features_server = select_features(X_train_server, y_train_server,
                                          server_objective, server_num_class,
                                          top_k=K_FEATURES)

    X_train_fs_server = X_train_server[top_features_server]
    X_valid_fs_server = X_valid_server[top_features_server]

    server_model = train_xgb(X_train_fs_server, y_train_server, X_valid_fs_server, y_valid_server,
                             server_objective, server_num_class)

    # --- 9. è©•ä¼°æ¨¡å‹ ä¸¦ç´€éŒ„ ---
    print("\nğŸ“Š è©•ä¼° *æœ€çµ‚* æ¨¡å‹...")

    X_valid_fs_action = X_valid_action[top_features_action]
    X_valid_fs_point = X_valid_point[top_features_point]

    pred_action_val = actionid_model.predict(X_valid_fs_action)
    pred_point_val = pointid_model.predict(X_valid_fs_point)

    if y_server_all.nunique() > 2:
        pred_server_proba_val = server_model.predict_proba(X_valid_fs_server)
        auc_server = roc_auc_score(y_valid_server, pred_server_proba_val, multi_class="ovr")
    else:
        pred_server_proba_val = server_model.predict_proba(X_valid_fs_server)[:, 1]
        auc_server = roc_auc_score(y_valid_server, pred_server_proba_val)

    f1_action = f1_score(y_valid_action, pred_action_val, average="macro")
    f1_point = f1_score(y_valid_point, pred_point_val, average="macro")
    weighted_score = 0.4 * f1_action + 0.4 * f1_point + 0.2 * auc_server

    print(f"actionId Macro F1: {f1_action:.4f}")
    print(f"pointId  Macro F1: {f1_point:.4f}")
    print(f"serverGetPoint AUC: {auc_server:.4f}")
    print(f"ç¶œåˆè©•åˆ†: {weighted_score:.4f}")

    # --- ğŸŒŸ 9.1 ç¹ªè£½ä¸¦å„²å­˜æ··æ·†çŸ©é™£ (NEW) ---
    print("\nğŸ¨ æ­£åœ¨ç”¢ç”Ÿæ··æ·†çŸ©é™£åœ–...")
    # å–å¾— actionId çš„æ‰€æœ‰é¡åˆ¥æ¨™ç±¤ä¸¦æ’åº
    action_labels = sorted(y_action_all.unique())
    plot_confusion_matrix(y_valid_action, pred_action_val,
                          class_labels=action_labels,
                          title='ActionID Confusion Matrix (Validation Set)',
                          filename='confusion_matrix_action.png')

    # å–å¾— pointId çš„æ‰€æœ‰é¡åˆ¥æ¨™ç±¤ä¸¦æ’åº
    point_labels = sorted(y_point_all.unique())
    plot_confusion_matrix(y_valid_point, pred_point_val,
                          class_labels=point_labels,
                          title='PointID Confusion Matrix (Validation Set)',
                          filename='confusion_matrix_point.png')

    # --- 9.5 ç´€éŒ„å¯¦é©—çµæœ ---
    results_to_log = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "f1_action": f"{f1_action:.4f}",
        "f1_point": f"{f1_point:.4f}",
        "auc_server": f"{auc_server:.4f}",
        "weighted_score": f"{weighted_score:.4f}",
        "K_FEATURES": K_FEATURES,
        "n_iter_search": N_ITER_SEARCH,
        "action_weights_adj": json.dumps(action_weight_adjustments),
        "point_weights_adj": json.dumps(point_weight_adjustments)
    }
    log_experiment_results(LOG_FILE, results_to_log)

    # --- 10. ç”¢ç”Ÿé æ¸¬ ---
    print("\nğŸ§® ç”¢ç”Ÿæ¸¬è©¦é æ¸¬ä¸­...")
    X_test_fs_action = X_test.reindex(columns=X_train_action.columns, fill_value=0)[top_features_action]
    X_test_fs_point = X_test.reindex(columns=X_train_point.columns, fill_value=0)[top_features_point]
    X_test_fs_server = X_test.reindex(columns=X_train_server.columns, fill_value=0)[top_features_server]

    pred_action_test_raw = actionid_model.predict(X_test_fs_action)
    pred_point_test_raw = pointid_model.predict(X_test_fs_point)

    if y_server_all.nunique() > 2:
        pred_server_test_raw = server_model.predict(X_test_fs_server)
        pred_server = revert_negative(pred_server_test_raw, "serverGetPoint", original_max_labels)
    else:
        pred_server = server_model.predict_proba(X_test_fs_server)[:, 1] # æ©Ÿç‡

    # é‚„åŸ -1
    pred_action = revert_negative(pred_action_test_raw, "actionId", original_max_labels)
    pred_point = revert_negative_pointid(pred_point_test_raw, original_max_labels.get("pointId"))

    # --- 11. å„²å­˜æäº¤æª”æ¡ˆ ---
    save_submission(test_last_shot, pred_action, pred_point, pred_server, SAMPLE_SUB_PATH, SUBMISSION_PATH)

if __name__ == "__main__":
    main()