#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ“ å¤šä»»å‹™åˆ†é¡æ¨¡å‹ (é‡æ§‹ç‰ˆ v2.8) - ä¸»åŸ·è¡Œæª”
---------------------------------------------------------------------
ğŸŒŸ v2.8 æ›´æ–°ï¼š
- ä¿®æ­£ v2.7 åˆä½µé¡åˆ¥å¾Œ [8, 9, 14, 17, 18] -> 8 é€ æˆçš„æ¨™ç±¤ä¸é€£çºŒ (ValueError)ã€‚
- æ–°å¢ actionId æ¨™ç±¤é‡æ–°ç·¨ç¢¼ (Label Re-encoding) é‚è¼¯ã€‚
- ç¢ºä¿æ¨™ç±¤åœ¨ (1) è¨“ç·´ (2) æ¬Šé‡èª¿æ•´ (3) æ··æ·†çŸ©é™£ (4) æœ€çµ‚é æ¸¬ å››å€‹éšæ®µéƒ½èƒ½æ­£ç¢ºå°æ‡‰ã€‚
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import RandomizedSearchCV, PredefinedSplit
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix
from sklearn.feature_selection import VarianceThreshold
from sklearn.utils.class_weight import compute_sample_weight
from tqdm import tqdm
import csv
import os
from datetime import datetime
import json
import matplotlib.pyplot as plt
import seaborn as sns


# å¾ data_processing.py åŒ¯å…¥æ‰€æœ‰è³‡æ–™è™•ç†å‡½å¼
from data_processing import (
    load_data,
    create_features,
    preprocess,
    create_training_data,
    create_group_split
)

# =========================================================
# 6ï¸âƒ£ ç¨ç«‹ç‰¹å¾µé¸å–
# =========================================================
def select_features(X, y, objective, num_class=None, top_k=30):
    """ä½¿ç”¨ XGBoost å…ˆè¨“ç·´ä¸€è¼ªï¼Œé¸å‡ºæœ€é‡è¦çš„å‰ K å€‹ç‰¹å¾µã€‚"""
    selector = VarianceThreshold(threshold=0.0)
    X_var = selector.fit_transform(X)
    selected_cols = X.columns[selector.get_support()]
    X_var = pd.DataFrame(X_var, columns=selected_cols, index=X.index)

    model_params = {
        "objective": objective, "eval_metric": "mlogloss" if "multi" in objective else "logloss",
        "learning_rate": 0.1, "max_depth": 5, "n_estimators": 100,
        "subsample": 0.8, "colsample_bytree": 0.8,
        "random_state": 42, "tree_method": "hist"
    }
    if num_class is not None:
        model_params["num_class"] = num_class

    model_tmp = xgb.XGBClassifier(**model_params)
    model_tmp.fit(X_var, y)

    importances = model_tmp.feature_importances_
    importance_df = pd.DataFrame({"feature": selected_cols, "importance": importances}).sort_values("importance", ascending=False)
    return importance_df.head(top_k)["feature"].tolist()

def select_features_xgb(X, y, num_class, top_k=40, objective="multi:softmax"):
    """èˆ‡ select_features é¡ä¼¼ï¼Œä½†ç‚º RandomizedSearch æµç¨‹å®¢è£½åŒ–"""
    selector = VarianceThreshold(threshold=0.0)
    X_var = selector.fit_transform(X)
    selected_cols = X.columns[selector.get_support()]
    X_var = pd.DataFrame(X_var, columns=selected_cols, index=X.index)

    model_params = {
        "objective": objective, "eval_metric": "mlogloss", "learning_rate": 0.1,
        "max_depth": 5, "n_estimators": 100, "subsample": 0.8, "colsample_bytree": 0.8,
        "random_state": 42, "tree_method": "hist", "num_class": num_class
    }
    model_tmp = xgb.XGBClassifier(**model_params)
    model_tmp.fit(X_var, y)
    importances = model_tmp.feature_importances_
    importance_df = pd.DataFrame({"feature": selected_cols, "importance": importances}).sort_values("importance", ascending=False)
    return importance_df.head(top_k)["feature"].tolist()


# =========================================================
# 7ï¸âƒ£ XGBoost è¨“ç·´å‡½å¼
# =========================================================
def train_xgb(X_train, y_train, X_valid, y_valid, objective, num_class=None):
    """è¨“ç·´ XGBoost æ¨¡å‹çš„é€šç”¨å‡½å¼ï¼ˆç”¨æ–¼ serverGetPointï¼‰"""
    params = {
        "objective": objective, "eval_metric": "mlogloss" if "multi" in objective else "logloss",
        "learning_rate": 0.05, "max_depth": 9, "subsample": 0.9,
        "colsample_bytree": 0.9, "n_estimators": 100, "random_state": 42,
        "tree_method": "hist", "early_stopping_rounds": 30
    }
    if num_class is not None:
        params["num_class"] = num_class

    model = xgb.XGBClassifier(**params)
    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)
    return model

def train_xgb_with_search(X_train, X_valid, y_train, y_valid, num_class, top_features,
                            objective="multi:softmax", n_iter=25,
                            custom_weight_adjustments=None):
    """
    ç”¨ RandomizedSearchCV + class_weight + early stopping è¨“ç·´ XGBoost
    """
    X_train_fs, X_valid_fs = X_train[top_features], X_valid[top_features]
    X_search, y_search = pd.concat([X_train_fs, X_valid_fs]), pd.concat([y_train, y_valid])

    ps = PredefinedSplit([-1] * len(X_train_fs) + [0] * len(X_valid_fs))

    print("  > æ­£åœ¨ä½¿ç”¨ 'balanced' è‡ªå‹•æ¬Šé‡")
    # é€™è£¡çš„ y_search å·²ç¶“æ˜¯é‡æ–°ç·¨ç¢¼éçš„ 0..k-1 æ¨™ç±¤
    search_weights = compute_sample_weight(class_weight='balanced', y=y_search)

    if custom_weight_adjustments:
        print(f"  > æ­£åœ¨å¾®èª¿æ¬Šé‡ (å·²è½‰æ›ç‚ºæ–°æ¨™ç±¤): {custom_weight_adjustments}")
        temp_weights_df = pd.DataFrame({'label': y_search, 'weight': search_weights})
        for label, multiplier in custom_weight_adjustments.items():
            # label å·²ç¶“æ˜¯ 0..k-1 çš„æ–°æ¨™ç±¤
            temp_weights_df.loc[temp_weights_df['label'] == label, 'weight'] *= multiplier
        search_weights = temp_weights_df['weight'].values

    fit_params = {"eval_set": [(X_valid_fs, y_valid)], "verbose": False}

    if xgb.__version__ >= "2.0.0":
        valid_weights = compute_sample_weight(class_weight='balanced', y=y_valid)

        if custom_weight_adjustments:
            temp_valid_weights_df = pd.DataFrame({'label': y_valid, 'weight': valid_weights})
            for label, multiplier in custom_weight_adjustments.items():
                temp_valid_weights_df.loc[temp_valid_weights_df['label'] == label, 'weight'] *= multiplier
            valid_weights = temp_valid_weights_df['weight'].values

        fit_params["sample_weight_eval_set"] = [valid_weights]

    param_dist = {
        'learning_rate': [0.05, 0.1, 0.15, 0.2], 'max_depth': [3, 5, 7, 9],
        'n_estimators': [100, 200, 300, 400], 'subsample': [0.7, 0.8, 0.9],
        'colsample_bytree': [0.7, 0.8, 0.9], 'gamma': [0, 0.1, 0.2]
    }
    base_model = xgb.XGBClassifier(objective=objective, eval_metric="mlogloss", random_state=42, tree_method="hist", num_class=num_class, early_stopping_rounds=30)

    rand_search = RandomizedSearchCV(estimator=base_model, param_distributions=param_dist, n_iter=n_iter, scoring='f1_macro', cv=ps, n_jobs=-1, verbose=2, random_state=42)

    rand_search.fit(
        X_search,
        y_search,
        sample_weight=search_weights,
        **fit_params
    )

    print(f"âœ… {objective} æœ€ä½³åƒæ•¸: {rand_search.best_params_}")
    print(f"âœ… {objective} æœ€ä½³ F1 Macro (Val): {rand_search.best_score_:.4f}")
    return rand_search.best_estimator_

# =========================================================
# ğŸŒŸ 1ï¸âƒ£0ï¸âƒ£ ç¹ªè£½æ··æ·†çŸ©é™£ (NEW)
# =========================================================
def plot_confusion_matrix(y_true, y_pred, class_labels, title, filename):
    """ç¹ªè£½ä¸¦å„²å­˜æ··æ·†çŸ©é™£åœ–"""
    cm = confusion_matrix(y_true, y_pred, labels=class_labels)
    plt.figure(figsize=(14, 12))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_labels, yticklabels=class_labels)
    plt.title(title, fontsize=16)
    plt.ylabel('True Label', fontsize=12)
    plt.xlabel('Predicted Label', fontsize=12)
    plt.tight_layout()
    plt.savefig(filename)
    plt.close() # é—œé–‰åœ–åƒä»¥é‡‹æ”¾è¨˜æ†¶é«”
    print(f"âœ… å·²å„²å­˜æ··æ·†çŸ©é™£åœ–: {filename}")

# =========================================================
# 1ï¸âƒ£1ï¸âƒ£ è¼¸å‡º submission.csv
# =========================================================
def revert_negative(pred, col_name, original_max_labels_dict):
    """å°‡ max+1 é¡åˆ¥è½‰å› -1"""
    # (æ­¤å‡½å¼ç¾åœ¨æ¥æ”¶çš„æ˜¯ "åŸå§‹" æ¨™ç±¤ï¼Œå› ç‚ºæˆ‘å€‘åœ¨é æ¸¬å¾Œå·²åè½‰)
    if col_name in original_max_labels_dict:
        replacement_val = original_max_labels_dict[col_name]
        pred = pd.Series(pred)
        pred[pred == replacement_val] = -1
        return pred.values
    return pred

def revert_negative_pointid(pred, replacement_val):
    """å°‡ max+1 é¡åˆ¥è½‰å› -1ï¼ˆfor pointIdï¼‰"""
    if replacement_val is not None:
        pred = pd.Series(pred)
        pred[pred == replacement_val] = -1
        return pred.values
    return pred

def save_submission(test_last_shot, pred_action, pred_point, pred_server, sample_path, output_path):
    """å„²å­˜æäº¤æª”æ¡ˆ"""
    submission = pd.DataFrame({"rally_uid": test_last_shot["rally_uid"], "serverGetPoint": pred_server, "pointId": pred_point, "actionId": pred_action})
    try:
        sample_sub = pd.read_csv(sample_path)
        submission = submission[sample_sub.columns]
    except FileNotFoundError:
        print(f"âš ï¸ æ‰¾ä¸åˆ° {sample_path}ï¼Œå°‡ä½¿ç”¨é è¨­æ¬„ä½é †åºã€‚")
    except Exception as e:
        print(f"âš ï¸ è®€å– {sample_path} æ™‚å‡ºéŒ¯: {e}")
    submission.to_csv(output_path, index=False)
    print(f"\nâœ… å·²è¼¸å‡º {output_path}\nSubmission shape: {submission.shape}\n{submission.head()}")

# =========================================================
# 1ï¸âƒ£2ï¸âƒ£ å¯¦é©—çµæœç´€éŒ„
# =========================================================
def log_experiment_results(log_path, results_dict):
    """å°‡å–®æ¬¡å¯¦é©—çµæœ (å­—å…¸) é™„åŠ åˆ° CSV æª”æ¡ˆä¸­"""
    try:
        loggable_dict = {}
        for key, value in results_dict.items():
            if value is None:
                loggable_dict[key] = "None"
            elif isinstance(value, dict):
                 loggable_dict[key] = json.dumps(value)
            else:
                loggable_dict[key] = value

        fieldnames = loggable_dict.keys()
        file_exists = os.path.isfile(log_path)

        with open(log_path, 'a', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)

            if not file_exists:
                writer.writeheader()

            writer.writerow(loggable_dict)
        print(f"âœ… å¯¦é©—çµæœå·²ç´€éŒ„è‡³ {log_path}")
    except Exception as e:
        print(f"âš ï¸ ç´€éŒ„å¯¦é©—çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

# =========================================================
# ğŸš€ ä¸»åŸ·è¡Œæµç¨‹
# =========================================================
def main():
    # --- åƒæ•¸è¨­å®š ---
    K_FEATURES = 20
    N_ITER_SEARCH = 25
    LOG_FILE = "experiment_log.csv"

    TRAIN_PATH = "train.csv"
    TEST_PATH = "test.csv"
    SAMPLE_SUB_PATH = "sample_submission.csv"
    SUBMISSION_PATH = "submission.csv"

    # --- 1. è®€å– & 2. ç‰¹å¾µå·¥ç¨‹ ---
    train, test = load_data(TRAIN_PATH, TEST_PATH)

    # ğŸ”½ [ ç¬¬ 1 æ­¥ ] - åˆä½µ actionId é¡åˆ¥
    print("ğŸ”„ æ­£åœ¨åˆä½µ actionId é¡åˆ¥ [8, 9, 14, 17, 18] -> 8 ...")
    categories_to_merge = [9, 14, 17, 18] 
    target_category = 8                   
    train['actionId'] = train['actionId'].replace(categories_to_merge, target_category)
    test['actionId'] = test['actionId'].replace(categories_to_merge, target_category)
    print(f"âœ… actionId é¡åˆ¥åˆä½µå®Œæˆã€‚")

    print("âš™ï¸ æ­£åœ¨ç‚º train å»ºç«‹ç‰¹å¾µ...")
    train = create_features(train)
    print("âš™ï¸ æ­£åœ¨ç‚º test å»ºç«‹ç‰¹å¾µ...")
    test = create_features(test)

    # --- æ¬„ä½å°é½Š ---
    print("ğŸ”„ æ­£åœ¨å°é½Š Train å’Œ Test çš„æ¬„ä½...")
    train_cols = set(train.columns)
    test_cols = set(test.columns)
    missing_in_test = list(train_cols - test_cols)
    if missing_in_test:
        for c in missing_in_test:
            if c.startswith('type_'): test[c] = 0
    missing_in_train = list(test_cols - train_cols)
    if missing_in_train:
        for c in missing_in_train:
            if c.startswith('type_'): train[c] = 0
    common_cols = [col for col in train.columns if col in test.columns]
    test = test[common_cols]
    train = train[common_cols + list(train_cols - test_cols)]

    # --- 3. é è™•ç† ---
    target_cols = ["actionId", "pointId", "serverGetPoint"]
    drop_cols = ["rally_uid", "rally_id", "match", "numberGame"]
    feature_cols = [c for c in train.columns if c not in target_cols + drop_cols and c in test.columns]
    feature_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(train[c])]

    print(f"âœ… ä½¿ç”¨ {len(feature_cols)} å€‹ç‰¹å¾µé€²è¡Œè¨“ç·´ã€‚ ('sex' æ¬„ä½å·²ä¿ç•™)")

    # Preprocess æœƒè™•ç† -1 (å°‡å…¶è½‰æ›ç‚º max+1)
    train, test_last_shot, original_max_labels = preprocess(train, test)

    # --- 4. å»ºç«‹ N -> N+1 è¨“ç·´è³‡æ–™ & 5. Group Split ---
    X, y_action, y_point, y_server, rally_uids_for_split = create_training_data(train, feature_cols)
    X_test = test_last_shot[feature_cols].copy().fillna(0)
    X_test = X_test.reindex(columns=X.columns, fill_value=0)
    split_data, y_all = create_group_split(X, y_action, y_point, y_server, rally_uids_for_split)
    y_action_all, y_point_all, y_server_all = y_all


    # --- ğŸ”½ [ ç¬¬ 2 æ­¥ ] é‡æ–°ç·¨ç¢¼ actionId æ¨™ç±¤ (é—œéµ) ---
    print("ğŸ”„ æ­£åœ¨å°‡ actionId æ¨™ç±¤é‡æ–°ç·¨ç¢¼ç‚º 0 åˆ° k-1...")
    
    # 1. å–å¾—æ‰€æœ‰ *åˆä½µå¾Œ* ä¸” *preprocess è™•ç†é -1* çš„å”¯ä¸€æ¨™ç±¤
    #    (ä¾‹å¦‚ [0, 1, ... 8, 10, ... 19, 20])
    action_original_unique_labels = sorted(y_action_all.unique())
    
    # 2. å»ºç«‹ [ èˆŠæ¨™ç±¤ -> æ–°æ¨™ç±¤(0..k-1) ] çš„å°æ‡‰å­—å…¸
    action_label_map = {old_label: new_label for new_label, old_label in enumerate(action_original_unique_labels)}
    # 3. å»ºç«‹ [ æ–°æ¨™ç±¤(0..k-1) -> èˆŠæ¨™ç±¤ ] çš„åå‘å°æ‡‰å­—å…¸
    action_reverse_label_map = {new_label: old_label for old_label, new_label in action_label_map.items()}

    # 4. æ‡‰ç”¨é€™å€‹å°æ‡‰åˆ° y_all, y_train, y_valid
    y_action_all = y_action_all.map(action_label_map)
    y_train_action = split_data['action'][2].map(action_label_map)
    y_valid_action = split_data['action'][3].map(action_label_map)

    # 5. (é‡è¦) æ›´æ–° split_data å­—å…¸ä¸­çš„å€¼
    split_data['action'] = (split_data['action'][0], split_data['action'][1], y_train_action, y_valid_action)

    print(f"âœ… actionId é‡æ–°ç·¨ç¢¼å®Œæˆã€‚æ–°é¡åˆ¥æ•¸é‡: {len(action_original_unique_labels)}")
    print(f"   (ç¯„ä¾‹) èˆŠæ¨™ç±¤ 8 -> æ–°æ¨™ç±¤ {action_label_map.get(8)}")
    print(f"   (ç¯„ä¾‹) èˆŠæ¨™ç±¤ 10 -> æ–°æ¨™ç±¤ {action_label_map.get(10)}")
    print(f"   (ç¯„ä¾‹) èˆŠæ¨™ç±¤ 19 -> æ–°æ¨™ç±¤ {action_label_map.get(19)}")
    # --- ğŸ”¼ [ æ–°å¢ç¨‹å¼ç¢¼ ] - çµæŸ ---


    # --- 6 & 7. actionId æ¨¡å‹è¨“ç·´ (RandomizedSearchCV) ---
    X_train_action, X_valid_action, y_train_action, y_valid_action = split_data['action'] # é€™è£¡å·²æ˜¯é‡æ–°ç·¨ç¢¼éçš„
    num_class_action = y_action_all.nunique()
    print(f"âœ… actionId é¡åˆ¥æ•¸é‡: {num_class_action} (å·²åˆä½µä¸¦é‡æ–°ç·¨ç¢¼)")

    print(f"ğŸ§© ç‚º actionId é¸å–å‰ {K_FEATURES} å€‹ç‰¹å¾µ...")
    # (æ³¨æ„: y_train_action å·²ç¶“æ˜¯æ–°æ¨™ç±¤äº†)
    top_features_action = select_features_xgb(X_train_action, y_train_action, num_class_action, top_k=K_FEATURES)
    print(f"ğŸ”¥ actionId Top 5: {top_features_action[:5]}")

    # --- ğŸ”½ [ ç¬¬ 3 æ­¥ ] è½‰æ›æ¬Šé‡å°æ‡‰ ---
    original_action_weight_adj = { 19: 1.5} # é€™æ˜¯æ‚¨çš„åŸå§‹è¨­å®š
    action_weight_adjustments = {}
    if original_action_weight_adj:
        print(f"ğŸ”„ æ­£åœ¨è½‰æ› actionId æ¬Šé‡æ¨™ç±¤: {original_action_weight_adj}")
        for old_label, weight in original_action_weight_adj.items():
            if old_label in action_label_map:
                new_label = action_label_map[old_label]
                action_weight_adjustments[new_label] = weight
            else:
                # å¦‚æœ 19 å·²ç¶“è¢«åˆä½µæˆ–ä¸å­˜åœ¨æ–¼è³‡æ–™ä¸­ï¼Œé€™è£¡æœƒæç¤º
                print(f"âš ï¸ æ¬Šé‡è­¦å‘Š: åŸå§‹æ¨™ç±¤ {old_label} åœ¨é‡æ–°ç·¨ç¢¼å¾Œä¸å­˜åœ¨ï¼Œå°‡è¢«å¿½ç•¥ã€‚")
        print(f"âœ… è½‰æ›å¾Œçš„æ–°æ¬Šé‡ (ç”¨æ–¼è¨“ç·´): {action_weight_adjustments}")
    # --- ğŸ”¼ [ ä¿®æ”¹ç¨‹å¼ç¢¼ ] - çµæŸ ---

    print("ğŸš€ è¨“ç·´ actionId æ¨¡å‹ (RandomizedSearchCV)...")
    actionid_model = train_xgb_with_search(
        X_train_action, X_valid_action, y_train_action, y_valid_action,
        num_class_action, top_features_action,
        n_iter=N_ITER_SEARCH,
        custom_weight_adjustments=action_weight_adjustments # å‚³å…¥è½‰æ›å¾Œçš„æ–°æ¬Šé‡
    )

    # --- 6 & 7. pointId æ¨¡å‹è¨“ç·´ (RandomizedSearchCV) ---
    X_train_point, X_valid_point, y_train_point, y_valid_point = split_data['point']
    num_class_point = y_point_all.nunique()
    print(f"âœ… pointId é¡åˆ¥æ•¸é‡: {num_class_point}")

    print(f"ğŸ§© ç‚º pointId é¸å–å‰ {K_FEATURES} å€‹ç‰¹å¾µ...")
    top_features_point = select_features_xgb(X_train_point, y_train_point, num_class_point, top_k=K_FEATURES)
    print(f"ğŸ”¥ pointId Top 5: {top_features_point[:5]}")

    point_weight_adjustments = None

    print("ğŸš€ è¨“ç·´ pointId æ¨¡å‹ (RandomizedSearchCV)...")
    pointid_model = train_xgb_with_search(
        X_train_point, X_valid_point, y_train_point, y_valid_point,
        num_class_point, top_features_point,
        n_iter=N_ITER_SEARCH,
        custom_weight_adjustments=point_weight_adjustments
    )

    # --- 8. åƒ…è¨“ç·´ serverGetPoint æ¨¡å‹ ---
    print("ğŸš€ è¨“ç·´ serverGetPoint æ¨¡å‹ä¸­...")
    X_train_server, X_valid_server, y_train_server, y_valid_server = split_data['server']

    server_objective = "binary:logistic" if y_server_all.nunique() <= 2 else "multi:softmax"
    server_num_class = y_server_all.nunique() if y_server_all.nunique() > 2 else None

    top_features_server = select_features(X_train_server, y_train_server,
                                          server_objective, server_num_class,
                                          top_k=K_FEATURES)

    X_train_fs_server = X_train_server[top_features_server]
    X_valid_fs_server = X_valid_server[top_features_server]

    server_model = train_xgb(X_train_fs_server, y_train_server, X_valid_fs_server, y_valid_server,
                             server_objective, server_num_class)

    # --- 9. è©•ä¼°æ¨¡å‹ ä¸¦ç´€éŒ„ ---
    print("\nğŸ“Š è©•ä¼° *æœ€çµ‚* æ¨¡å‹...")

    X_valid_fs_action = X_valid_action[top_features_action]
    X_valid_fs_point = X_valid_point[top_features_point]

    pred_action_val = actionid_model.predict(X_valid_fs_action) # é æ¸¬çš„æ˜¯ "æ–°æ¨™ç±¤"
    pred_point_val = pointid_model.predict(X_valid_fs_point)

    if y_server_all.nunique() > 2:
        pred_server_proba_val = server_model.predict_proba(X_valid_fs_server)
        auc_server = roc_auc_score(y_valid_server, pred_server_proba_val, multi_class="ovr")
    else:
        pred_server_proba_val = server_model.predict_proba(X_valid_fs_server)[:, 1]
        auc_server = roc_auc_score(y_valid_server, pred_server_proba_val)

    # (æ³¨æ„: y_valid_action ä¹Ÿæ˜¯ "æ–°æ¨™ç±¤", pred_action_val ä¹Ÿæ˜¯ "æ–°æ¨™ç±¤", f1_score å¯ä»¥ç›´æ¥è¨ˆç®—)
    f1_action = f1_score(y_valid_action, pred_action_val, average="macro")
    f1_point = f1_score(y_valid_point, pred_point_val, average="macro")
    weighted_score = 0.4 * f1_action + 0.4 * f1_point + 0.2 * auc_server

    print(f"actionId Macro F1: {f1_action:.4f}")
    print(f"pointId  Macro F1: {f1_point:.4f}")
    print(f"serverGetPoint AUC: {auc_server:.4f}")
    print(f"ç¶œåˆè©•åˆ†: {weighted_score:.4f}")

    # --- ğŸŒŸ 9.1 ç¹ªè£½ä¸¦å„²å­˜æ··æ·†çŸ©é™£ (NEW) ---
    print("\nğŸ¨ æ­£åœ¨ç”¢ç”Ÿæ··æ·†çŸ©é™£åœ–...")
    
    # --- ğŸ”½ [ ç¬¬ 4 æ­¥ ] æ··æ·†çŸ©é™£ä½¿ç”¨ "åŸå§‹" æ¨™ç±¤ ---
    # 1. å–å¾— actionId çš„ *åŸå§‹* é¡åˆ¥æ¨™ç±¤ (æˆ‘å€‘ä¹‹å‰å­˜çš„)
    action_labels = action_original_unique_labels
    
    # 2. æˆ‘å€‘éœ€è¦å°‡ y_valid (æ–°æ¨™ç±¤) å’Œ pred_val (æ–°æ¨™ç±¤) åè½‰å› "åŸå§‹æ¨™ç±¤"
    y_valid_action_original = y_valid_action.map(action_reverse_label_map)
    pred_action_val_original = pd.Series(pred_action_val).map(action_reverse_label_map).values
    
    plot_confusion_matrix(y_valid_action_original, pred_action_val_original,
                          class_labels=action_labels,
                          title='ActionID Confusion Matrix (Validation Set)',
                          filename='confusion_matrix_action.png')
    # --- ğŸ”¼ [ ä¿®æ”¹ç¨‹å¼ç¢¼ ] - çµæŸ ---

    # å–å¾— pointId çš„æ‰€æœ‰é¡åˆ¥æ¨™ç±¤ä¸¦æ’åº (pointId ä¸å—å½±éŸ¿)
    point_labels = sorted(y_point_all.unique())
    plot_confusion_matrix(y_valid_point, pred_point_val,
                          class_labels=point_labels,
                          title='PointID Confusion Matrix (Validation Set)',
                          filename='confusion_matrix_point.png')

    # --- 9.5 ç´€éŒ„å¯¦é©—çµæœ ---
    results_to_log = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "f1_action": f"{f1_action:.4f}",
        "f1_point": f"{f1_point:.4f}",
        "auc_server": f"{auc_server:.4f}",
        "weighted_score": f"{weighted_score:.4f}",
        "K_FEATURES": K_FEATURES,
        "n_iter_search": N_ITER_SEARCH,
        "action_weights_adj": json.dumps(original_action_weight_adj), # ç´€éŒ„åŸå§‹æ¬Šé‡
        "point_weights_adj": json.dumps(point_weight_adjustments)
    }
    log_experiment_results(LOG_FILE, results_to_log)

    # --- 10. ç”¢ç”Ÿé æ¸¬ ---
    print("\nğŸ§® ç”¢ç”Ÿæ¸¬è©¦é æ¸¬ä¸­...")
    X_test_fs_action = X_test.reindex(columns=X_train_action.columns, fill_value=0)[top_features_action]
    X_test_fs_point = X_test.reindex(columns=X_train_point.columns, fill_value=0)[top_features_point]
    X_test_fs_server = X_test.reindex(columns=X_train_server.columns, fill_value=0)[top_features_server]

    # 1. æ¨¡å‹é æ¸¬çš„æ˜¯ "æ–°æ¨™ç±¤" (0..k-1)
    pred_action_test_raw_mapped = actionid_model.predict(X_test_fs_action)
    
    # --- ğŸ”½ [ ç¬¬ 5 æ­¥ ] åè½‰ actionId é æ¸¬æ¨™ç±¤ ---
    print("ğŸ”„ æ­£åœ¨å°‡ actionId é æ¸¬çµæœåè½‰å›åŸå§‹æ¨™ç±¤...")
    # 2. å°‡ "æ–°æ¨™ç±¤" è½‰å› "èˆŠæ¨™ç±¤" (ä¾‹å¦‚ [17] -> [20])
    pred_action_test_raw = pd.Series(pred_action_test_raw_mapped).map(action_reverse_label_map).values
    # --- ğŸ”¼ [ ä¿®æ”¹ç¨‹å¼ç¢¼ ] - çµæŸ ---

    pred_point_test_raw = pointid_model.predict(X_test_fs_point)

    if y_server_all.nunique() > 2:
        pred_server_test_raw = server_model.predict(X_test_fs_server)
        pred_server = revert_negative(pred_server_test_raw, "serverGetPoint", original_max_labels)
    else:
        pred_server = server_model.predict_proba(X_test_fs_server)[:, 1] # æ©Ÿç‡

    # é‚„åŸ -1
    # 3. é€™è£¡çš„ pred_action_test_raw å·²ç¶“æ˜¯ "èˆŠæ¨™ç±¤" äº†
    #    revert_negative æœƒæ­£ç¢ºåœ° (ä¾‹å¦‚) å°‡ 20 è½‰å› -1
    pred_action = revert_negative(pred_action_test_raw, "actionId", original_max_labels)
    pred_point = revert_negative_pointid(pred_point_test_raw, original_max_labels.get("pointId"))

    # --- 11. å„²å­˜æäº¤æª”æ¡ˆ ---
    save_submission(test_last_shot, pred_action, pred_point, pred_server, SAMPLE_SUB_PATH, SUBMISSION_PATH)

if __name__ == "__main__":
    main()