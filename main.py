#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ“ å¤šä»»å‹™åˆ†é¡æ¨¡å‹ (é‡æ§‹ç‰ˆ v2)ï¼šPredict actionId / pointId / serverGetPoint
---------------------------------------------------------------------
ğŸŒŸ v2 æ›´æ–°ï¼š
- æ–°å¢ `create_features` å‡½å¼ã€‚
- åŠ å…¥ 3 æ‹çš„æ»¯å¾Œç‰¹å¾µ (prev_1, prev_2, prev_3)ã€‚
- åŠ å…¥ `score_diff` æƒ…å¢ƒç‰¹å¾µã€‚
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split, RandomizedSearchCV, PredefinedSplit
from sklearn.metrics import f1_score, roc_auc_score
from sklearn.feature_selection import VarianceThreshold
from sklearn.utils.class_weight import compute_sample_weight
from tqdm import tqdm
import sys

# =========================================================
# 1ï¸âƒ£ è³‡æ–™è®€å–
# =========================================================
def load_data(train_path="train.csv", test_path="test.csv"):
    """è®€å–è¨“ç·´é›†å’Œæ¸¬è©¦é›†"""
    try:
        train = pd.read_csv(train_path)
        test = pd.read_csv(test_path)
        print(f"âœ… Train shape: {train.shape}")
        print(f"âœ… Test shape: {test.shape}")
        return train, test
    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° {train_path} æˆ– {test_path}ã€‚è«‹ç¢ºä¿æª”æ¡ˆåœ¨æ­£ç¢ºçš„è·¯å¾‘ã€‚")
        sys.exit(1)

# =========================================================
# ğŸŒŸ 2ï¸âƒ£ ç‰¹å¾µå·¥ç¨‹ (NEW)
# =========================================================
def create_features(df):
    """ç‚º train å’Œ test æ•¸æ“šé›†å»ºç«‹æ–°çš„åºåˆ—ç‰¹å¾µ (æ»¯å¾Œç‰¹å¾µ)"""
    df_new = df.copy()
    
    # ç¢ºä¿è³‡æ–™æŒ‰å›åˆå’Œæ‹æ•¸æ’åº
    df_new = df_new.sort_values(by=['rally_uid', 'strickNumber'])
    
    # 1. æ»¯å¾Œç‰¹å¾µ (Lag Features)
    # é¸æ“‡è¦æ»¯å¾Œçš„æ¬„ä½
    lag_cols = ['actionId', 'pointId', 'spinId', 'strengthId', 'positionId']
    
    # å»ºç«‹ N-1, N-2, N-3 çš„æ»¯å¾Œç‰¹å¾µ
    print(f"  > æ­£åœ¨å»ºç«‹ N-1, N-2, N-3 æ»¯å¾Œç‰¹å¾µ...")
    for col in lag_cols:
        for n in [1, 2, 3]:
            # .shift(n) ç²å– (N-n) æ‹çš„æ•¸æ“š
            df_new[f'prev_{n}_{col}'] = df_new.groupby('rally_uid')[col].shift(n)

    # 2. æƒ…å¢ƒç‰¹å¾µ (Context Features) - åˆ†æ•¸
    df_new['score_diff'] = df_new['scoreSelf'] - df_new['scoreOther']

    # ğŸŒŸ å¡«å…… shift() ç”¢ç”Ÿçš„ NaNs
    # ç”¨ -1 å¡«å……ï¼Œä»¥å€åˆ¥æ–¼ 0 (0 å¯èƒ½æ˜¯ä¸€å€‹æœ‰æ•ˆçš„ ID)
    fill_cols = [col for col in df_new.columns if 'prev_' in col]
    df_new[fill_cols] = df_new[fill_cols].fillna(-1) 

    return df_new

# =========================================================
# 3ï¸âƒ£ é è™•ç†
# =========================================================
def preprocess(train_df, test_df):
    """
    1. ä¿®æ­£ -1 é¡åˆ¥å•é¡Œ
    2. å–å¾—æ¸¬è©¦é›†æœ€å¾Œä¸€ç­†è³‡æ–™
    """
    # ğŸŒŸ (1/6) é æ¸¬æ™‚ï¼šä½¿ç”¨æ¯å€‹ rally_uid çš„ "æœ€å¾Œä¸€ç­†" è³‡æ–™
    test_last_shot = test_df.groupby('rally_uid').tail(1).copy()
    print(f"âœ… Test (last shots) shape: {test_last_shot.shape}")

    # ä¿®æ­£ -1 é¡åˆ¥
    original_max_labels = {}
    for col in ["actionId", "pointId", "serverGetPoint"]:
        if col in train_df.columns and (train_df[col] == -1).any():
            max_label = train_df[col].max()
            original_max_labels[col] = max_label + 1
            print(f"âš ï¸ {col} å«æœ‰ -1ï¼Œå°‡å…¶æ›¿æ›ç‚º {max_label + 1}")
            train_df[col] = train_df[col].replace(-1, max_label + 1)
    
    return train_df, test_last_shot, original_max_labels

# =========================================================
# 4ï¸âƒ£ å»ºç«‹è¨“ç·´ä»»å‹™ (N -> N+1)
# =========================================================
def create_training_data(train_df, feature_cols):
    """
    é‡æ–°å®šç¾©è¨“ç·´ä»»å‹™ (N -> N+1)
    - ç‰¹å¾µ (X) æ˜¯ç•¶å‰æ“Šçƒ (Shot N)
    - æ¨™ç±¤ (y) æ˜¯ "ä¸‹ä¸€çƒ" (Shot N+1)
    """
    # ç‰¹å¾µ (X) æ˜¯ç•¶å‰æ“Šçƒ (Shot N)
    X = train_df[feature_cols].copy().fillna(0) # ğŸŒŸ æå‰å¡«å…… NaN

    # æ¨™ç±¤ (y) æ˜¯ "ä¸‹ä¸€çƒ" (Shot N+1)
    y_action = train_df.groupby('rally_uid')['actionId'].shift(-1)
    y_point = train_df.groupby('rally_uid')['pointId'].shift(-1)
    y_server = train_df['serverGetPoint'] # serverGetPoint æ˜¯å›åˆçµæœï¼Œä¸éœ€ shift

    # å„²å­˜ rally_uid ä»¥ä¾¿é€²è¡Œ group split
    rally_uids_for_split = train_df['rally_uid']

    # ğŸŒŸ åˆªé™¤æ²’æœ‰ "ä¸‹ä¸€çƒ" çš„è¡Œ (å³æ¯å€‹å›åˆçš„æœ€å¾Œä¸€çƒ)
    valid_indices = y_action.notna() & y_point.notna()
    X = X[valid_indices]
    y_action = y_action[valid_indices]
    y_point = y_point[valid_indices]
    y_server = y_server[valid_indices]
    rally_uids_for_split = rally_uids_for_split[valid_indices]

    print(f"âœ… é‡æ–°å»ºç«‹è¨“ç·´é›† (N -> N+1)ï¼Œæ–° shape: {X.shape}")
    
    return X, y_action, y_point, y_server, rally_uids_for_split

# =========================================================
# 5ï¸âƒ£ å»ºç«‹ç„¡æ´©æ¼çš„é©—è­‰é›† (Group Split)
# =========================================================
def create_group_split(X, y_action, y_point, y_server, rally_uids):
    """
    ä½¿ç”¨ Group Split å»ºç«‹ç„¡æ´©æ¼çš„é©—è­‰é›†
    """
    print("ğŸ§© å»ºç«‹ç„¡æ´©æ¼çš„é©—è­‰é›†ä¸­ (Group Split)...")
    unique_rallies = rally_uids.unique()
    train_rallies, valid_rallies = train_test_split(unique_rallies, test_size=0.2, random_state=42)

    train_mask = rally_uids.isin(train_rallies)
    valid_mask = rally_uids.isin(valid_rallies)

    # å»ºç«‹ train/valid è³‡æ–™é›†
    data = {
        'action': (X[train_mask], X[valid_mask], y_action[train_mask], y_action[valid_mask]),
        'point': (X[train_mask], X[valid_mask], y_point[train_mask], y_point[valid_mask]),
        'server': (X[train_mask], X[valid_mask], y_server[train_mask], y_server[valid_mask])
    }
    
    return data, (y_action, y_point, y_server) # å›å‚³ y_all ä»¥ä¾¿è¨ˆç®— nunique

# =========================================================
# 6ï¸âƒ£ ç¨ç«‹ç‰¹å¾µé¸å–
# =========================================================
def select_features(X, y, objective, num_class=None, top_k=30):
    """
    ä½¿ç”¨ XGBoost å…ˆè¨“ç·´ä¸€è¼ªï¼Œé¸å‡ºæœ€é‡è¦çš„å‰ K å€‹ç‰¹å¾µã€‚
    """
    selector = VarianceThreshold(threshold=0.0)
    X_var = selector.fit_transform(X)
    selected_cols = X.columns[selector.get_support()]
    
    # ç¢ºä¿ X_var æ˜¯ DataFrame
    X_var = pd.DataFrame(X_var, columns=selected_cols, index=X.index)

    # ğŸŒŸ è¨­å®šæ¨¡å‹åƒæ•¸
    model_params = {
        "objective": objective,
        "eval_metric": "mlogloss" if "multi" in objective else "logloss",
        "learning_rate": 0.1, "max_depth": 5, "n_estimators": 100,
        "subsample": 0.8, "colsample_bytree": 0.8,
        "random_state": 42, "tree_method": "hist"
    }
    if num_class is not None:
        model_params["num_class"] = num_class

    model_tmp = xgb.XGBClassifier(**model_params)
    model_tmp.fit(X_var, y)

    importances = model_tmp.feature_importances_
    importance_df = pd.DataFrame({
        "feature": selected_cols,
        "importance": importances
    }).sort_values("importance", ascending=False)

    top_features = importance_df.head(top_k)["feature"].tolist()
    return top_features

def apply_feature_selection(split_data, y_all, X_test, K_FEATURES):
    """
    ç‚ºä¸‰å€‹ç›®æ¨™åˆ†åˆ¥é€²è¡Œç‰¹å¾µé¸å–
    """
    X_train_action, X_valid_action, y_train_action, _ = split_data['action']
    X_train_point, X_valid_point, y_train_point, _ = split_data['point']
    X_train_server, X_valid_server, y_train_server, _ = split_data['server']
    
    y_action_all, y_point_all, y_server_all = y_all

    # --- ç‚º actionId é¸å–ç‰¹å¾µ ---
    print(f"ğŸ§© ç‚º actionId é¸å–å‰ {K_FEATURES} å€‹ç‰¹å¾µ...")
    top_features_action = select_features(X_train_action, y_train_action, 
                                          objective="multi:softmax", 
                                          num_class=y_action_all.nunique(), 
                                          top_k=K_FEATURES)
    print(f"ğŸ”¥ actionId Top 5: {top_features_action[:5]}")

    # --- ç‚º pointId é¸å–ç‰¹å¾µ ---
    print(f"ğŸ§© ç‚º pointId é¸å–å‰ {K_FEATURES} å€‹ç‰¹å¾µ...")
    top_features_point = select_features(X_train_point, y_train_point, 
                                         objective="multi:softmax",
                                         num_class=y_point_all.nunique(),
                                         top_k=K_FEATURES)
    print(f"ğŸ”¥ pointId Top 5: {top_features_point[:5]}")

    # --- ç‚º serverGetPoint é¸å–ç‰¹å¾µ ---
    print(f"ğŸ§© ç‚º serverGetPoint é¸å–å‰ {K_FEATURES} å€‹ç‰¹å¾µ...")
    if y_train_server.nunique() > 2:
        server_objective = "multi:softmax"
        server_num_class = y_server_all.nunique()
    else:
        server_objective = "binary:logistic"
        server_num_class = None

    top_features_server = select_features(X_train_server, y_train_server,
                                          objective=server_objective,
                                          num_class=server_num_class,
                                          top_k=K_FEATURES)
    print(f"ğŸ”¥ serverGetPoint Top 5: {top_features_server[:5]}")

    # å»ºç«‹æœ€çµ‚çš„ç‰¹å¾µé›†
    fs_data = {
        'action': (X_train_action[top_features_action], X_valid_action[top_features_action], X_test[top_features_action]),
        'point': (X_train_point[top_features_point], X_valid_point[top_features_point], X_test[top_features_point]),
        'server': (X_train_server[top_features_server], X_valid_server[top_features_server], X_test[top_features_server])
    }
    
    return fs_data

# =========================================================
# 7ï¸âƒ£ XGBoost è¨“ç·´å‡½å¼
# =========================================================
def train_xgb(X_train, y_train, X_valid, y_valid, objective, num_class=None):
    """è¨“ç·´ XGBoost æ¨¡å‹çš„é€šç”¨å‡½å¼"""
    params = {
        "objective": objective,
        "eval_metric": "mlogloss" if "multi" in objective else "logloss",
        "learning_rate": 0.05,
        "max_depth": 9, # éµç…§å‰ä¸€ç‰ˆä¿®æ­£ (6 -> 5)
        "subsample": 0.9,
        "colsample_bytree": 0.9,
        "n_estimators": 100,
        "random_state": 42,
        "tree_method": "hist",
        "early_stopping_rounds": 30 # éµç…§å‰ä¸€ç‰ˆä¿®æ­£ (20 -> 30)
    }
    if num_class is not None:
        params["num_class"] = num_class

    model = xgb.XGBClassifier(**params)
    model.fit(
        X_train,
        y_train,
        eval_set=[(X_valid, y_valid)],
        verbose=False
    )
    return model

def train_xgb_with_search(X_train, X_valid, y_train, y_valid, num_class, top_features, objective="multi:softmax", n_iter=25):
    """
    ç”¨ RandomizedSearchCV + class_weight + early stopping è¨“ç·´ XGBoost å¤šåˆ†é¡æ¨¡å‹
    """
    X_train_fs = X_train[top_features]
    X_valid_fs = X_valid[top_features]

    X_search = pd.concat([X_train_fs, X_valid_fs])
    y_search = pd.concat([y_train, y_valid])

    test_fold = np.zeros(len(X_search))
    test_fold[:len(X_train_fs)] = -1
    ps = PredefinedSplit(test_fold)

    search_weights = compute_sample_weight(class_weight='balanced', y=y_search)

    fit_params = {
        "eval_set": [(X_valid_fs, y_valid)],
        "verbose": False
    }
    if xgb.__version__ >= "2.0.0":
        valid_weights = compute_sample_weight(class_weight='balanced', y=y_valid)
        fit_params["sample_weight_eval_set"] = [valid_weights]

    param_dist = {
        'learning_rate': [0.05, 0.1, 0.15, 0.2],
        'max_depth': [3, 5, 7, 9],
        'n_estimators': [100, 200, 300, 400],
        'subsample': [0.7, 0.8, 0.9],
        'colsample_bytree': [0.7, 0.8, 0.9],
        'gamma': [0, 0.1, 0.2]
    }

    base_model = xgb.XGBClassifier(
        objective=objective,
        eval_metric="mlogloss",
        random_state=42,
        tree_method="hist",
        num_class=num_class,
        early_stopping_rounds=30
    )

    rand_search = RandomizedSearchCV(
        estimator=base_model,
        param_distributions=param_dist,
        n_iter=n_iter,
        scoring='f1_macro',
        cv=ps,
        n_jobs=-1,
        verbose=2,
        random_state=42
    )

    rand_search.fit(
        X_search,
        y_search,
        sample_weight=search_weights,
        **fit_params
    )

    print(f"âœ… {objective} æœ€ä½³åƒæ•¸: {rand_search.best_params_}")
    print(f"âœ… {objective} æœ€ä½³ F1 Macro (Val): {rand_search.best_score_:.4f}")

    return rand_search.best_estimator_

def select_features_xgb(X, y, num_class, top_k=40, objective="multi:softmax"):
    selector = VarianceThreshold(threshold=0.0)
    X_var = selector.fit_transform(X)
    selected_cols = X.columns[selector.get_support()]
    X_var = pd.DataFrame(X_var, columns=selected_cols, index=X.index)

    model_params = {
        "objective": objective,
        "eval_metric": "mlogloss",
        "learning_rate": 0.1, "max_depth": 5, "n_estimators": 100,
        "subsample": 0.8, "colsample_bytree": 0.8,
        "random_state": 42, "tree_method": "hist",
        "num_class": num_class
    }
    model_tmp = xgb.XGBClassifier(**model_params)
    model_tmp.fit(X_var, y)
    importances = model_tmp.feature_importances_
    importance_df = pd.DataFrame({
        "feature": selected_cols,
        "importance": importances
    }).sort_values("importance", ascending=False)
    top_features = importance_df.head(top_k)["feature"].tolist()
    return top_features

def revert_negative_pointid(pred, replacement_val):
    """å°‡ max+1 é¡åˆ¥è½‰å› -1ï¼ˆfor pointIdï¼‰"""
    if replacement_val is not None:
        pred = pd.Series(pred)
        pred[pred == replacement_val] = -1
        return pred.values
    return pred

# =========================================================
# 8ï¸âƒ£ ä¸‰å€‹æ¨¡å‹è¨“ç·´
# =========================================================
def train_all_models(fs_data, split_data, y_all):
    """
    ä½¿ç”¨å„è‡ªé¸å–çš„ç‰¹å¾µé›†è¨“ç·´ä¸‰å€‹æ¨¡å‹
    """
    models = {}
    
    # å–å¾—æ¨™ç±¤
    _, _, y_train_action, y_valid_action = split_data['action']
    _, _, y_train_point, y_valid_point = split_data['point']
    _, _, y_train_server, y_valid_server = split_data['server']
    
    # å–å¾—ç‰¹å¾µ
    X_train_fs_action, X_valid_fs_action, _ = fs_data['action']
    X_train_fs_point, X_valid_fs_point, _ = fs_data['point']
    X_train_fs_server, X_valid_fs_server, _ = fs_data['server']
    
    y_action_all, y_point_all, y_server_all = y_all

    print("ğŸš€ è¨“ç·´ actionId æ¨¡å‹ä¸­...")
    models['action'] = train_xgb(X_train_fs_action, y_train_action, 
                                 X_valid_fs_action, y_valid_action,
                                 objective="multi:softmax", num_class=y_action_all.nunique())

    print("ğŸš€ è¨“ç·´ pointId æ¨¡å‹ä¸­...")
    models['point'] = train_xgb(X_train_fs_point, y_train_point,
                                X_valid_fs_point, y_valid_point,
                                objective="multi:softmax", num_class=y_point_all.nunique())

    print("ğŸš€ è¨“ç·´ serverGetPoint æ¨¡å‹ä¸­...")
    if y_server_all.nunique() > 2:
        print("âš ï¸ serverGetPoint ç™¼ç¾å¤šæ–¼2å€‹é¡åˆ¥ï¼Œä½¿ç”¨ multi:softmax")
        models['server'] = train_xgb(X_train_fs_server, y_train_server,
                                      X_valid_fs_server, y_valid_server,
                                      objective="multi:softmax", num_class=y_server_all.nunique())
    else:
        models['server'] = train_xgb(X_train_fs_server, y_train_server,
                                      X_valid_fs_server, y_valid_server,
                                      objective="binary:logistic")
                                      
    return models

# =========================================================
# 9ï¸âƒ£ æ¨¡å‹è©•ä¼°
# =========================================================
def evaluate_models(models, fs_data, split_data, y_all):
    """
    åœ¨é©—è­‰é›†ä¸Šè©•ä¼°æ¨¡å‹
    """
    # å–å¾—æ¨™ç±¤
    _, _, _, y_valid_action = split_data['action']
    _, _, _, y_valid_point = split_data['point']
    _, _, _, y_valid_server = split_data['server']
    
    # å–å¾—ç‰¹å¾µ
    _, X_valid_fs_action, _ = fs_data['action']
    _, X_valid_fs_point, _ = fs_data['point']
    _, X_valid_fs_server, _ = fs_data['server']

    y_server_all = y_all[2]
    
    # é æ¸¬
    pred_action = models['action'].predict(X_valid_fs_action)
    pred_point = models['point'].predict(X_valid_fs_point)

    if y_server_all.nunique() > 2:
        pred_server_proba = models['server'].predict_proba(X_valid_fs_server)
        auc_server = roc_auc_score(y_valid_server, pred_server_proba, multi_class="ovr")
    else:
        pred_server_proba = models['server'].predict_proba(X_valid_fs_server)[:, 1]
        auc_server = roc_auc_score(y_valid_server, pred_server_proba)

    f1_action = f1_score(y_valid_action, pred_action, average="macro")
    f1_point = f1_score(y_valid_point, pred_point, average="macro")

    print("\nğŸ“Š Validation Results (Fixed):")
    print(f"actionId Macro F1: {f1_action:.4f}")
    print(f"pointId  Macro F1: {f1_point:.4f}")
    print(f"serverGetPoint AUC: {auc_server:.4f}")

    score = 0.4 * f1_action + 0.4 * f1_point + 0.2 * auc_server
    print(f"ç¶œåˆè©•åˆ†: {score:.4f}")

# =========================================================
# ğŸ”Ÿ æ¸¬è©¦é›†é æ¸¬ & æ¨™ç±¤é‚„åŸ
# =========================================================
def revert_negative(pred, col_name, original_max_labels_dict):
    """å°‡ max+1 é¡åˆ¥è½‰å› -1"""
    if col_name in original_max_labels_dict:
        replacement_val = original_max_labels_dict[col_name]
        pred = pd.Series(pred)
        pred[pred == replacement_val] = -1
        return pred.values
    return pred

def generate_predictions(models, fs_data, y_all, original_max_labels):
    """
    ç”¢ç”Ÿæ¸¬è©¦é›†é æ¸¬ä¸¦é‚„åŸ -1 æ¨™ç±¤
    """
    print("\nğŸ§® ç”¢ç”Ÿæ¸¬è©¦é æ¸¬ä¸­...")
    
    _, _, X_test_fs_action = fs_data['action']
    _, _, X_test_fs_point = fs_data['point']
    _, _, X_test_fs_server = fs_data['server']
    
    y_server_all = y_all[2]

    # é æ¸¬
    pred_action_test = models['action'].predict(X_test_fs_action)
    pred_point_test = models['point'].predict(X_test_fs_point)

    if y_server_all.nunique() > 2:
        pred_server_test_labels = models['server'].predict(X_test_fs_server)
    else:
        pred_server_test_proba = models['server'].predict_proba(X_test_fs_server)[:, 1]

    # é‚„åŸ -1
    pred_action_test = revert_negative(pred_action_test, "actionId", original_max_labels)
    pred_point_test = revert_negative(pred_point_test, "pointId", original_max_labels)

    if y_server_all.nunique() > 2:
        pred_server_final = revert_negative(pred_server_test_labels, "serverGetPoint", original_max_labels)
    else:
        pred_server_final = pred_server_test_proba # æ©Ÿç‡ä¸ç”¨é‚„åŸ
        
    return pred_action_test, pred_point_test, pred_server_final

# =========================================================
# 1ï¸âƒ£1ï¸âƒ£ è¼¸å‡º submission.csv
# =========================================================
def save_submission(test_last_shot, pred_action, pred_point, pred_server, 
                    sample_path="sample_submission.csv", output_path="submission.csv"):
    """
    å„²å­˜æäº¤æª”æ¡ˆ
    """
    submission = pd.DataFrame({
        "rally_uid": test_last_shot["rally_uid"],
        "serverGetPoint": pred_server,
        "pointId": pred_point,
        "actionId": pred_action
    })

    try:
        sample_sub = pd.read_csv(sample_path)
        submission = submission[sample_sub.columns]
    except FileNotFoundError:
        print(f"âš ï¸ æ‰¾ä¸åˆ° {sample_path}ï¼Œå°‡ä½¿ç”¨é è¨­æ¬„ä½é †åºã€‚")
    except Exception as e:
        print(f"âš ï¸ è®€å– {sample_path} æ™‚å‡ºéŒ¯: {e}")

    submission.to_csv(output_path, index=False)
    print(f"\nâœ… å·²è¼¸å‡º {output_path}")
    print(f"Submission shape: {submission.shape}")
    print(submission.head())

# =========================================================
# ğŸš€ ä¸»åŸ·è¡Œæµç¨‹
# =========================================================
def main():
    # --- åƒæ•¸è¨­å®š ---
    K_FEATURES = 20
    TRAIN_PATH = "train.csv"
    TEST_PATH = "test.csv"
    SAMPLE_SUB_PATH = "sample_submission.csv"
    SUBMISSION_PATH = "submission.csv"

    # --- 1. è®€å–è³‡æ–™ ---
    train, test = load_data(TRAIN_PATH, TEST_PATH)

    # --- 2. ç‰¹å¾µå·¥ç¨‹ ---
    print("âš™ï¸ æ­£åœ¨ç‚º train å»ºç«‹æ»¯å¾Œç‰¹å¾µ...")
    train = create_features(train)
    print("âš™ï¸ æ­£åœ¨ç‚º test å»ºç«‹æ»¯å¾Œç‰¹å¾µ...")
    test = create_features(test)

    # --- 3. é è™•ç† ---
    target_cols = ["actionId", "pointId", "serverGetPoint"]
    drop_cols = ["rally_uid", "rally_id"]
    feature_cols = [c for c in train.columns if c not in target_cols + drop_cols and c in test.columns]
    print(f"âœ… ä½¿ç”¨ {len(feature_cols)} å€‹ç‰¹å¾µé€²è¡Œè¨“ç·´ã€‚")

    train, test_last_shot, original_max_labels = preprocess(train, test)

    # --- 4. å»ºç«‹ N -> N+1 è¨“ç·´è³‡æ–™ ---
    X, y_action, y_point, y_server, rally_uids_for_split = create_training_data(train, feature_cols)
    X_test = test_last_shot[feature_cols].copy().fillna(0)

    # --- 5. å»ºç«‹ Group Split ---
    split_data, y_all = create_group_split(X, y_action, y_point, y_server, rally_uids_for_split)

    # --- 5.1 actionId split ---
    X_train_action, X_valid_action, y_train_action, y_valid_action = split_data['action']
    num_class_action = y_all[0].nunique()
    print(f"âœ… actionId é¡åˆ¥æ•¸é‡: {num_class_action}")

    # --- 6. ç‰¹å¾µé¸å– (for actionId) ---
    print(f"ğŸ§© ç‚º actionId é¸å–å‰ {K_FEATURES} å€‹ç‰¹å¾µ...")
    top_features_action = select_features_xgb(X_train_action, y_train_action, num_class_action, top_k=K_FEATURES)
    print(f"ğŸ”¥ actionId Top 5: {top_features_action[:5]}")

    # --- 7. è¨“ç·´ actionId æ¨¡å‹ (RandomizedSearchCV) ---
    print("ğŸš€ è¨“ç·´ actionId æ¨¡å‹ (RandomizedSearchCV)...")
    actionid_model = train_xgb_with_search(X_train_action, X_valid_action, y_train_action, y_valid_action, num_class_action, top_features_action)

    # --- 5.1 pointId split ---
    X_train_point, X_valid_point, y_train_point, y_valid_point = split_data['point']
    num_class_point = y_all[1].nunique()
    print(f"âœ… pointId é¡åˆ¥æ•¸é‡: {num_class_point}")

    # --- 6. ç‰¹å¾µé¸å– (for pointId) ---
    print(f"ğŸ§© ç‚º pointId é¸å–å‰ {K_FEATURES} å€‹ç‰¹å¾µ...")
    top_features_point = select_features_xgb(X_train_point, y_train_point, num_class_point, top_k=K_FEATURES)
    print(f"ğŸ”¥ pointId Top 5: {top_features_point[:5]}")

    # --- 7. è¨“ç·´ pointId æ¨¡å‹ (RandomizedSearchCV) ---
    print("ğŸš€ è¨“ç·´ pointId æ¨¡å‹ (RandomizedSearchCV)...")
    pointid_model = train_xgb_with_search(X_train_point, X_valid_point, y_train_point, y_valid_point, num_class_point, top_features_point)

    # --- 8. serverGetPoint ç”¨åŸæœ¬æµç¨‹ï¼ˆæˆ–åŒæ¨£æµç¨‹ï¼Œè¦–éœ€æ±‚ï¼‰ ---
    # ç”¨åŸæœ¬ main.py çš„æµç¨‹
    fs_data = apply_feature_selection(split_data, y_all, X_test, K_FEATURES)
    models = train_all_models(fs_data, split_data, y_all)

    # --- 9. è©•ä¼°æ¨¡å‹ ---
    evaluate_models(models, fs_data, split_data, y_all)

    # --- 10. ç”¢ç”Ÿé æ¸¬ ---
    # actionId/serverGetPoint ç”¨åŸæœ¬æµç¨‹
    pred_action, _, pred_server = generate_predictions(models, fs_data, y_all, original_max_labels)

    # pointId ç”¨æ–°æ¨¡å‹
    X_test_point_fs = X_test[top_features_point]
    pred_point_test = pointid_model.predict(X_test_point_fs)
    pred_point_test = revert_negative_pointid(pred_point_test, original_max_labels.get("pointId", None))

    # --- 11. å„²å­˜æäº¤æª”æ¡ˆ ---
    save_submission(test_last_shot, pred_action, pred_point_test, pred_server,
                    SAMPLE_SUB_PATH, SUBMISSION_PATH)

if __name__ == "__main__":
    main()

