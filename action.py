#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ“ å–®ä»»å‹™åˆ†é¡æ¨¡å‹ (actionId)ï¼šåƒ…é æ¸¬ actionId
---------------------------------------------------------------------
ğŸŒŸ ä¾†æºï¼š
- å¾ v2 å¤šä»»å‹™æ¨¡å‹é‡æ§‹è€Œä¾†ï¼Œå°ˆæ³¨æ–¼ actionId é æ¸¬ã€‚
- ä¿ç•™äº† v2 çš„æ»¯å¾Œç‰¹å¾µ (prev_1, prev_2, prev_3) å’Œ score_diff ç‰¹å¾µã€‚

â­ v3 æ›´æ–°ï¼š
- æ•´åˆ RandomizedSearchCV é€²è¡Œè¶…åƒæ•¸èª¿å„ªã€‚
- ä¿ç•™ Group Split (ä½¿ç”¨ PredefinedSplit)ã€‚
- æ•´åˆ class_weight ('balanced') è™•ç†ä¸å¹³è¡¡å•é¡Œã€‚
- æ•´åˆ Early Stopping æå‡æœå°‹æ•ˆç‡ã€‚

ğŸ Debug ç­†è¨˜ï¼š
1.  **[æœ€å¯èƒ½çš„éŒ¯èª¤] UnicodeEncodeError**ï¼š
    å¦‚æœä½ çš„ä½œæ¥­ç³»çµ± (ç‰¹åˆ¥æ˜¯ Windows) çš„
    consoleï¼ˆå‘½ä»¤æç¤ºå­—å…ƒï¼‰é è¨­ç·¨ç¢¼ä¸æ˜¯ UTF-8ï¼Œ
    åŸ·è¡Œ `print("âœ… æœå°‹å®Œæˆ!")` 
    é€™é¡åŒ…å«ä¸­æ–‡çš„æŒ‡ä»¤æ™‚ï¼Œå¯èƒ½æœƒå¼•ç™¼ `UnicodeEncodeError`ã€‚

    **è§£æ±ºæ–¹æ³•**ï¼š
    åœ¨åŸ·è¡Œæ­¤è…³æœ¬å‰ï¼Œå…ˆåœ¨ä½ çš„çµ‚ç«¯æ©Ÿè¨­å®šç’°å¢ƒè®Šæ•¸ï¼š
    - (Windows CMD): `set PYTHONIOENCODING=utf-8`
    - (Windows PowerShell): `$env:PYTHONIOENCODING = "utf-8"`
    - (Linux/macOS): `export PYTHONIOENCODING=utf-8`
    ç„¶å¾Œå†åŸ·è¡Œ `python your_script_name.py`

2.  **[æ¸…é™¤] ç§»é™¤æœªä½¿ç”¨çš„å¥—ä»¶**ï¼š
    ç§»é™¤äº† `from tqdm import tqdm`ï¼Œå› ç‚º `RandomizedSearchCV(verbose=2)` 
    å·²ç¶“æä¾›äº†è¶³å¤ çš„é€²åº¦é¡¯ç¤ºã€‚
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split, RandomizedSearchCV, PredefinedSplit
from sklearn.metrics import f1_score
from sklearn.feature_selection import VarianceThreshold
from sklearn.utils.class_weight import compute_sample_weight
# from tqdm import tqdm  # (Debug) ç§»é™¤æœªä½¿ç”¨çš„ import
import sys

# =========================================================
# 1ï¸âƒ£ è³‡æ–™è®€å– (ç„¡è®Šå‹•)
# =========================================================
def load_data(train_path="train.csv", test_path="test.csv"):
    """è®€å–è¨“ç·´é›†å’Œæ¸¬è©¦é›†"""
    try:
        train = pd.read_csv(train_path)
        test = pd.read_csv(test_path)
        print(f"âœ… Train shape: {train.shape}")
        print(f"âœ… Test shape: {test.shape}")
        return train, test
    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° {train_path} æˆ– {test_path}ã€‚è«‹ç¢ºä¿æª”æ¡ˆåœ¨æ­£ç¢ºçš„è·¯å¾‘ã€‚")
        sys.exit(1)

# =========================================================
# 2ï¸âƒ£ ç‰¹å¾µå·¥ç¨‹ (ç„¡è®Šå‹•)
# =========================================================
def create_features(df):
    """ç‚º train å’Œ test æ•¸æ“šé›†å»ºç«‹æ–°çš„åºåˆ—ç‰¹å¾µ (æ»¯å¾Œç‰¹å¾µ)"""
    df_new = df.copy()
    
    # ç¢ºä¿è³‡æ–™æŒ‰å›åˆå’Œæ‹æ•¸æ’åº
    df_new = df_new.sort_values(by=['rally_uid', 'strickNumber'])
    
    # 1. æ»¯å¾Œç‰¹å¾µ (Lag Features)
    lag_cols = ['actionId', 'pointId', 'spinId', 'strengthId', 'positionId']
    
    print(f"   > æ­£åœ¨å»ºç«‹ N-1, N-2, N-3 æ»¯å¾Œç‰¹å¾µ...")
    for col in lag_cols:
        for n in [1, 2, 3]:
            df_new[f'prev_{n}_{col}'] = df_new.groupby('rally_uid')[col].shift(n)

    # 2. æƒ…å¢ƒç‰¹å¾µ (Context Features) - åˆ†æ•¸
    df_new['score_diff'] = df_new['scoreSelf'] - df_new['scoreOther']

    # å¡«å…… shift() ç”¢ç”Ÿçš„ NaNs
    fill_cols = [col for col in df_new.columns if 'prev_' in col]
    df_new[fill_cols] = df_new[fill_cols].fillna(-1) 

    return df_new

# =========================================================
# 3ï¸âƒ£ é è™•ç† (ç„¡è®Šå‹•)
# =========================================================
def preprocess(train_df, test_df):
    """
    1. ä¿®æ­£ actionId çš„ -1 é¡åˆ¥å•é¡Œ
    2. å–å¾—æ¸¬è©¦é›†æœ€å¾Œä¸€ç­†è³‡æ–™
    """
    # é æ¸¬æ™‚ï¼šä½¿ç”¨æ¯å€‹ rally_uid çš„ "æœ€å¾Œä¸€ç­†" è³‡æ–™
    test_last_shot = test_df.groupby('rally_uid').tail(1).copy()
    print(f"âœ… Test (last shots) shape: {test_last_shot.shape}")

    # ä¿®æ­£ -1 é¡åˆ¥ (åƒ…é‡å° actionId)
    original_max_label = None
    if (train_df["actionId"] == -1).any():
        max_label = train_df["actionId"].max()
        original_max_label = max_label + 1
        print(f"âš ï¸ actionId å«æœ‰ -1ï¼Œå°‡å…¶æ›¿æ›ç‚º {original_max_label}")
        train_df["actionId"] = train_df["actionId"].replace(-1, original_max_label)
    
    return train_df, test_last_shot, original_max_label

# =========================================================
# 4ï¸âƒ£ å»ºç«‹è¨“ç·´ä»»å‹™ (N -> N+1) (ç„¡è®Šå‹•)
# =========================================================
def create_training_data(train_df, feature_cols):
    """
    é‡æ–°å®šç¾©è¨“ç·´ä»»å‹™ (N -> N+1)
    - ç‰¹å¾µ (X) æ˜¯ç•¶å‰æ“Šçƒ (Shot N)
    - æ¨™ç±¤ (y) æ˜¯ "ä¸‹ä¸€çƒ" çš„ actionId (Shot N+1)
    """
    # ç‰¹å¾µ (X) æ˜¯ç•¶å‰æ“Šçƒ (Shot N)
    X = train_df[feature_cols].copy().fillna(0)

    # æ¨™ç±¤ (y) æ˜¯ "ä¸‹ä¸€çƒ" (Shot N+1) çš„ actionId
    y = train_df.groupby('rally_uid')['actionId'].shift(-1)
    
    # å„²å­˜ rally_uid ä»¥ä¾¿é€²è¡Œ group split
    rally_uids_for_split = train_df['rally_uid']

    # åˆªé™¤æ²’æœ‰ "ä¸‹ä¸€çƒ" çš„è¡Œ (å³æ¯å€‹å›åˆçš„æœ€å¾Œä¸€çƒ)
    valid_indices = y.notna()
    X = X[valid_indices]
    y = y[valid_indices]
    rally_uids_for_split = rally_uids_for_split[valid_indices]

    print(f"âœ… é‡æ–°å»ºç«‹è¨“ç·´é›† (N -> N+1)ï¼Œæ–° shape: {X.shape}")
    
    return X, y.astype(int), rally_uids_for_split

# =========================================================
# 5ï¸âƒ£ å»ºç«‹ç„¡æ´©æ¼çš„é©—è­‰é›† (Group Split) (ç„¡è®Šå‹•)
# =========================================================
def create_group_split(X, y, rally_uids):
    """
    ä½¿ç”¨ Group Split å»ºç«‹ç„¡æ´©æ¼çš„é©—è­‰é›†
    """
    print("ğŸ§© å»ºç«‹ç„¡æ´©æ¼çš„é©—è­‰é›†ä¸­ (Group Split)...")
    unique_rallies = rally_uids.unique()
    train_rallies, valid_rallies = train_test_split(unique_rallies, test_size=0.2, random_state=42)

    train_mask = rally_uids.isin(train_rallies)
    valid_mask = rally_uids.isin(valid_rallies)

    X_train, X_valid = X[train_mask], X[valid_mask]
    y_train, y_valid = y[train_mask], y[valid_mask]
    
    return X_train, X_valid, y_train, y_valid

# =========================================================
# 6ï¸âƒ£ ç‰¹å¾µé¸å– (ç„¡è®Šå‹•)
# =========================================================
def select_features(X, y, objective, num_class=None, top_k=30):
    """
    ä½¿ç”¨ XGBoost å…ˆè¨“ç·´ä¸€è¼ªï¼Œé¸å‡ºæœ€é‡è¦çš„å‰ K å€‹ç‰¹å¾µã€‚
    """
    selector = VarianceThreshold(threshold=0.0)
    X_var = selector.fit_transform(X)
    selected_cols = X.columns[selector.get_support()]
    
    X_var = pd.DataFrame(X_var, columns=selected_cols, index=X.index)

    model_params = {
        "objective": objective,
        "eval_metric": "mlogloss",
        "learning_rate": 0.1, "max_depth": 5, "n_estimators": 100,
        "subsample": 0.8, "colsample_bytree": 0.8,
        "random_state": 42, "tree_method": "hist",
        "num_class": num_class
    }

    model_tmp = xgb.XGBClassifier(**model_params)
    model_tmp.fit(X_var, y)

    importances = model_tmp.feature_importances_
    importance_df = pd.DataFrame({
        "feature": selected_cols,
        "importance": importances
    }).sort_values("importance", ascending=False)

    top_features = importance_df.head(top_k)["feature"].tolist()
    return top_features

# =========================================================
# 7ï¸âƒ£ æ¨™ç±¤é‚„åŸ 
# =========================================================
def revert_negative(pred, replacement_val):
    """å°‡ max+1 é¡åˆ¥è½‰å› -1"""
    if replacement_val is not None:
        pred = pd.Series(pred)
        pred[pred == replacement_val] = -1
        return pred.values
    return pred

# =========================================================
# 8ï¸âƒ£ è¼¸å‡º submission.csv 
# =========================================================
def save_submission(test_last_shot, pred_action, sample_path="sample_submission.csv", output_path="submission.csv"):
    """
    è®€å– sample_submissionï¼Œåƒ…æ›´æ–° actionId æ¬„ä½å¾Œå„²å­˜
    """
    try:
        submission = pd.read_csv(sample_path)
    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° {sample_path}ã€‚ç„¡æ³•å»ºç«‹æäº¤æª”æ¡ˆã€‚")
        return
        
    # ç¢ºä¿ rally_uid å°é½Š
    submission = submission.set_index('rally_uid')
    
    # å»ºç«‹ä¸€å€‹åŒ…å«é æ¸¬çµæœçš„ Seriesï¼Œä¸¦ä»¥ rally_uid ç‚ºç´¢å¼•
    pred_df = pd.DataFrame({
        "rally_uid": test_last_shot["rally_uid"],
        "actionId": pred_action
    }).set_index('rally_uid')

    # æ›´æ–° actionId æ¬„ä½
    submission['actionId'].update(pred_df['actionId'])
    submission['actionId'] = submission['actionId'].astype(int)

    # æ¢å¾©ç´¢å¼•ä¸¦å„²å­˜
    submission.reset_index(inplace=True)
    submission.to_csv(output_path, index=False)
    print(f"\nâœ… å·²è¼¸å‡º {output_path}")
    print(f"Submission shape: {submission.shape}")
    print(submission.head())

# =========================================================
# ğŸš€ ä¸»åŸ·è¡Œæµç¨‹
# =========================================================
def main():
    # --- åƒæ•¸è¨­å®š ---
    K_FEATURES = 40
    N_ITER_SEARCH = 25 # RandomizedSearch çš„æœå°‹æ¬¡æ•¸
    TRAIN_PATH = "train.csv"
    TEST_PATH = "test.csv"
    SAMPLE_SUB_PATH = "sample_submission.csv"
    SUBMISSION_PATH = "submission.csv"

    # --- 1. è®€å–è³‡æ–™ ---
    train, test = load_data(TRAIN_PATH, TEST_PATH)

    # --- 2. ç‰¹å¾µå·¥ç¨‹ ---
    print("âš™ï¸ æ­£åœ¨ç‚º train å»ºç«‹æ»¯å¾Œç‰¹å¾µ...")
    train = create_features(train)
    print("âš™ï¸ æ­£åœ¨ç‚º test å»ºç«‹æ»¯å¾Œç‰¹å¾µ...")
    test = create_features(test)

    # --- 3. é è™•ç† ---
    target_cols = ["actionId", "pointId", "serverGetPoint"]
    drop_cols = ["rally_uid", "rally_id"] 
    feature_cols = [c for c in train.columns if c not in target_cols + drop_cols and c in test.columns]
    print(f"âœ… ä½¿ç”¨ {len(feature_cols)} å€‹ç‰¹å¾µé€²è¡Œè¨“ç·´ã€‚")
    
    train, test_last_shot, original_max_label = preprocess(train, test)

    # --- 4. å»ºç«‹ N -> N+1 è¨“ç·´è³‡æ–™ ---
    X, y, rally_uids_for_split = create_training_data(train, feature_cols)
    X_test = test_last_shot[feature_cols].copy().fillna(0)

    # --- 5. å»ºç«‹ Group Split ---
    X_train, X_valid, y_train, y_valid = create_group_split(X, y, rally_uids_for_split)
    num_class = y.nunique()

    # --- 6. ç‰¹å¾µé¸å– ---
    print(f"ğŸ§© ç‚º actionId é¸å–å‰ {K_FEATURES} å€‹ç‰¹å¾µ...")
    # æ³¨æ„ï¼šç‰¹å¾µé¸å–åœ¨ X_train ä¸Šé€²è¡Œï¼Œä»¥é¿å…éæ“¬åˆ
    top_features = select_features(X_train, y_train, 
                                      objective="multi:softmax", 
                                      num_class=num_class, 
                                      top_k=K_FEATURES)
    print(f"ğŸ”¥ actionId Top 5: {top_features[:5]}")
    
    X_train_fs = X_train[top_features]
    X_valid_fs = X_valid[top_features]
    X_test_fs = X_test[top_features]

    # =========================================================
    # â­ 7. è¨­å®šè¶…åƒæ•¸æœå°‹ (RandomizedSearchCV)
    # =========================================================
    print("\nğŸš€ è¨­å®šè¶…åƒæ•¸æœå°‹ (RandomizedSearchCV)...")

    # 7a. å°‡è¨“ç·´é›†å’Œé©—è­‰é›†åˆä½µï¼Œä»¥ç¬¦åˆ PredefinedSplit çš„è¦æ±‚
    X_search = pd.concat([X_train_fs, X_valid_fs])
    y_search = pd.concat([y_train, y_valid])

    # 7b. å»ºç«‹ PredefinedSplit
    # -1 ä»£è¡¨è¨“ç·´é›†, 0 ä»£è¡¨é©—è­‰é›†
    test_fold = np.zeros(len(X_search))
    test_fold[:len(X_train_fs)] = -1
    ps = PredefinedSplit(test_fold)

    # 7c. ç‚ºæœå°‹è³‡æ–™è¨ˆç®— 'balanced' æ¬Šé‡
    print("   > æ­£åœ¨è¨ˆç®— 'balanced' é¡åˆ¥æ¬Šé‡ (for Search)...")
    search_weights = compute_sample_weight(
        class_weight='balanced',
        y=y_search
    )
    
    # 7d. ç‚º early stopping æº–å‚™ fit_params
    # **** DEBUG FIX ****
    # 'early_stopping_rounds' æ˜¯ XGBClassifier çš„ *constructor* åƒæ•¸ï¼Œ
    # è€Œä¸æ˜¯ .fit() æ–¹æ³•çš„åƒæ•¸ (åœ¨ scikit-learn æµç¨‹ä¸­)ã€‚
    # æˆ‘å€‘å°‡æŠŠå®ƒç§»è‡³ base_model çš„ constructor ä¸­ã€‚
    # .fit() åªéœ€è¦ eval_set å³å¯è§¸ç™¼ early stoppingã€‚
    fit_params = {
        # "early_stopping_rounds": 30, # <-- éŒ¯èª¤çš„æ”¾ç½®ä½ç½®
        "eval_set": [(X_valid_fs, y_valid)],
        "verbose": False
    }

    # æª¢æŸ¥ XGBoost ç‰ˆæœ¬æ˜¯å¦æ”¯æ´ eval_sample_weight
    if xgb.__version__ >= "2.0.0":
        print("   > åµæ¸¬åˆ° XGBoost >= 2.0.0ï¼Œå•Ÿç”¨ eval_sample_weightã€‚")
        valid_weights = compute_sample_weight(class_weight='balanced', y=y_valid)
        # **** DEBUG FIX ****
        # åƒæ•¸åç¨±æ‡‰ç‚º 'sample_weight_eval_set' è€Œä¸æ˜¯ 'eval_sample_weight'
        fit_params["sample_weight_eval_set"] = [valid_weights]
    else:
        print(f"   > è­¦å‘Š: XGBoost ç‰ˆæœ¬ ({xgb.__version__}) éèˆŠï¼Œç„¡æ³•ä½¿ç”¨ eval_sample_weightã€‚")

    # 7e. å®šç¾©åƒæ•¸ç¶²æ ¼ (param_distributions)
    param_dist = {
        'learning_rate': [0.05, 0.1, 0.15, 0.2],
        'max_depth': [3, 5, 7, 9],
        'n_estimators': [100, 200, 300, 400],
        'subsample': [0.7, 0.8, 0.9],
        'colsample_bytree': [0.7, 0.8, 0.9],
        'gamma': [0, 0.1, 0.2] # æ–°å¢ gamma åƒæ•¸
    }

    # 7f. å»ºç«‹åŸºæœ¬æ¨¡å‹
    base_model = xgb.XGBClassifier(
        objective="multi:softmax",
        eval_metric="mlogloss",
        random_state=42,
        tree_method="hist",
        num_class=num_class,
        early_stopping_rounds=30  # <-- DEBUG FIX: åƒæ•¸æ‡‰åœ¨æ­¤è™•
    )

    # 7g. å»ºç«‹ RandomizedSearchCV ç‰©ä»¶
    rand_search = RandomizedSearchCV(
        estimator=base_model,
        param_distributions=param_dist,
        n_iter=N_ITER_SEARCH,  # æœå°‹æ¬¡æ•¸
        scoring='f1_macro',    # æˆ‘å€‘çš„ç›®æ¨™æŒ‡æ¨™
        cv=ps,                 # ä½¿ç”¨æˆ‘å€‘è‡ªè¨‚çš„ (Train/Valid) åˆ‡åˆ†
        n_jobs=-1,             # ä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒ
        verbose=2,             # é¡¯ç¤ºæœå°‹é€²åº¦
        random_state=42
    )

    # =========================================================
    # â­ 8. åŸ·è¡Œæœå°‹èˆ‡è©•ä¼°
    # =========================================================
    print("\nğŸš€ é–‹å§‹åŸ·è¡Œè¶…åƒæ•¸æœå°‹...")
    
    # å°‡ search_weights å‚³éçµ¦ fit
    rand_search.fit(
        X_search, 
        y_search, 
        sample_weight=search_weights,
        **fit_params
    )

    print("\nğŸ“Š æœå°‹å®Œæˆ!")
    print(f"âœ… æœ€ä½³åƒæ•¸: {rand_search.best_params_}")
    print(f"âœ… æœ€ä½³ F1 Macro (Val): {rand_search.best_score_:.4f}")

    # å–å¾—æœ€ä½³æ¨¡å‹
    model = rand_search.best_estimator_
    

    # =========================================================
    # 9. ç”¢ç”Ÿé æ¸¬
    # =========================================================
    print("\nğŸ§® ç”¢ç”Ÿæ¸¬è©¦é æ¸¬ä¸­...")
    pred_test = model.predict(X_test_fs)
    pred_test_reverted = revert_negative(pred_test, original_max_label)
    
    # å„²å­˜
    save_submission(test_last_shot, pred_test_reverted, SAMPLE_SUB_PATH, SUBMISSION_PATH)
    
    print("\nğŸ‰ æµç¨‹åŸ·è¡Œå®Œç•¢ã€‚")

if __name__ == "__main__":
    main()


